{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nOuU_Zn-R3aY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('향수')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF0a0V4oSthG",
        "outputId": "72c12578-251f-4866-ca65-f25a6cc131be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "path = os.getcwd()\n",
        "\n",
        "print(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5lwxKCKS0YP",
        "outputId": "fed943ca-0dee-4485-fe6c-1bf7785678ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/향수\n"
          ]
        }
      ],
      "source": [
        "os.chdir(f\"{path}\" + \"/향수\")\n",
        "\n",
        "path2 = os.getcwd()\n",
        "print(path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQH0g0HuTOMd",
        "outputId": "21e0d25c-3e32-4204-a4b6-bf5a5477be80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  향수_노중복.zip\n",
            "   creating: 딥디크_도손/\n",
            "  inflating: 딥디크_도손/딥디크_도손_1.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_10.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_12.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_13.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_14.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_15.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_16.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_17.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_18.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_19.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_2.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_20.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_21.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_23.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_24.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_25.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_27.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_28.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_29.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_3.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_30.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_31.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_32.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_33.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_34.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_35.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_36.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_37.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_38.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_39.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_4.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_40.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_41.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_42.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_43.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_44.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_45.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_46.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_47.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_48.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_49.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_5.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_50.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_51.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_52.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_53.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_54.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_55.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_56.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_58.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_59.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_6.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_7.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_8.jpg  \n",
            "  inflating: 딥디크_도손/딥디크_도손_9.jpg  \n",
            "   creating: 르라보_상탈/\n",
            "  inflating: 르라보_상탈/르라보_상탈_1.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_10.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_11.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_12.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_13.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_14.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_15.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_16.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_17.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_18.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_19.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_2.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_20.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_21.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_23.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_24.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_25.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_26.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_27.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_28.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_29.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_3.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_30.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_31.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_32.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_33.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_35.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_36.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_37.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_38.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_39.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_4.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_40.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_42.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_45.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_46.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_47.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_48.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_5.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_51.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_52.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_53.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_54.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_56.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_57.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_58.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_6.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_60.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_7.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_8.jpg  \n",
            "  inflating: 르라보_상탈/르라보_상탈_9.jpg  \n",
            "   creating: 마르지엘라_커피브레이크/\n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_1.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_10.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_11.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_14.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_15.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_16.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_17.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_18.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_19.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_2.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_20.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_21.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_22.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_23.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_26.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_29.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_3.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_30.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_31.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_32.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_33.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_34.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_35.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_39.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_4.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_40.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_41.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_42.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_44.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_47.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_49.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_5.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_50.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_51.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_53.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_55.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_56.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_57.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_58.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_6.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_60.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_7.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_8.jpg  \n",
            "  inflating: 마르지엘라_커피브레이크/마르지엘라_커피브레이크_9.jpg  \n",
            "   creating: 바이레도_집시워터/\n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_1.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_10.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_11.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_12.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_14.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_16.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_17.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_18.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_19.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_2.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_20.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_21.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_22.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_23.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_25.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_26.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_27.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_29.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_3.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_30.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_31.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_32.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_34.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_35.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_37.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_38.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_4.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_41.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_43.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_44.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_46.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_47.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_48.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_5.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_51.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_52.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_53.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_54.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_55.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_56.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_57.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_58.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_59.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_6.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_60.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_7.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_8.jpg  \n",
            "  inflating: 바이레도_집시워터/바이레도_집시워터_9.jpg  \n",
            "   creating: 샤넬_블루드/\n",
            "  inflating: 샤넬_블루드/샤넬_블루드_1.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_10.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_11.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_12.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_13.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_14.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_15.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_16.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_17.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_18.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_19.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_2.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_20.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_21.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_22.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_24.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_25.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_26.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_28.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_29.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_3.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_30.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_31.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_32.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_33.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_34.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_35.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_36.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_37.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_38.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_39.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_4.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_40.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_41.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_42.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_43.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_44.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_45.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_46.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_47.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_48.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_49.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_5.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_50.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_51.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_53.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_54.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_56.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_57.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_58.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_59.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_6.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_60.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_7.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_8.jpg  \n",
            "  inflating: 샤넬_블루드/샤넬_블루드_9.jpg  \n",
            "   creating: 아쿠아디파르마_미르토/\n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_1.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_10.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_11.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_12.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_13.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_15.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_16.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_17.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_18.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_19.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_2.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_20.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_21.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_22.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_23.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_24.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_25.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_26.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_27.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_28.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_29.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_3.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_30.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_31.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_32.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_33.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_34.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_35.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_36.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_37.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_38.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_39.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_4.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_40.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_41.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_42.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_43.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_44.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_45.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_46.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_47.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_48.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_49.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_5.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_50.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_51.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_53.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_54.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_56.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_57.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_58.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_59.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_6.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_60.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_7.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_8.jpg  \n",
            "  inflating: 아쿠아디파르마_미르토/아쿠아디파르마_미르토_9.jpg  \n",
            "   creating: 에르메스_운 자르뎅 수르닐/\n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_1.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_10.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_11.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_12.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_13.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_14.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_15.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_16.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_17.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_18.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_19.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_2.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_20.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_21.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_22.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_23.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_24.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_25.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_27.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_28.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_29.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_3.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_30.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_31.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_32.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_33.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_34.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_35.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_36.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_37.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_39.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_4.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_40.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_41.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_42.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_43.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_44.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_45.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_46.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_47.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_48.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_49.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_5.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_50.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_52.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_53.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_56.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_58.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_59.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_6.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_60.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_7.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_8.jpg  \n",
            "  inflating: 에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_9.jpg  \n",
            "   creating: 이솝_테싯/\n",
            "  inflating: 이솝_테싯/이솝_테싯_1.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_10.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_11.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_12.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_13.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_14.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_16.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_17.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_18.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_2.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_20.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_21.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_22.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_23.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_24.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_25.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_26.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_27.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_28.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_29.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_3.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_30.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_31.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_32.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_33.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_34.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_35.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_36.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_37.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_38.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_39.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_4.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_40.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_41.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_42.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_43.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_44.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_45.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_46.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_48.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_49.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_5.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_50.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_52.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_54.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_55.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_56.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_57.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_58.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_59.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_6.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_60.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_7.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_8.jpg  \n",
            "  inflating: 이솝_테싯/이솝_테싯_9.jpg  \n",
            "   creating: 조말론_네롤리/\n",
            "  inflating: 조말론_네롤리/조말론_네롤리_1.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_10.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_12.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_13.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_14.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_15.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_16.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_17.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_19.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_2.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_20.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_21.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_22.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_23.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_24.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_25.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_26.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_27.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_28.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_29.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_3.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_30.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_31.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_32.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_33.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_34.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_35.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_36.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_38.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_39.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_4.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_40.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_41.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_42.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_43.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_44.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_45.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_46.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_47.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_48.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_49.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_5.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_50.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_51.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_52.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_53.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_54.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_55.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_56.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_57.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_58.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_6.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_60.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_7.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_8.jpg  \n",
            "  inflating: 조말론_네롤리/조말론_네롤리_9.jpg  \n",
            "   creating: 존바바토스_아티산/\n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_1.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_10.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_12.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_13.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_14.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_15.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_18.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_19.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_2.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_20.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_21.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_23.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_24.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_25.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_26.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_27.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_28.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_29.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_3.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_32.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_35.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_36.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_37.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_39.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_4.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_40.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_41.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_42.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_43.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_44.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_45.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_46.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_47.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_49.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_50.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_51.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_52.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_54.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_55.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_56.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_57.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_7.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_8.jpg  \n",
            "  inflating: 존바바토스_아티산/존바바토스_아티산_9.jpg  \n",
            "   creating: 크리드_로얄워터/\n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_1.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_10.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_11.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_12.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_13.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_14.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_15.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_16.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_17.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_18.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_19.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_2.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_20.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_21.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_22.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_25.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_27.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_28.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_29.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_3.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_30.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_31.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_32.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_33.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_34.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_35.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_36.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_37.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_38.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_39.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_4.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_42.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_44.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_45.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_46.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_47.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_49.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_5.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_50.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_51.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_52.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_53.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_55.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_56.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_57.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_58.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_59.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_6.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_60.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_7.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_8.jpg  \n",
            "  inflating: 크리드_로얄워터/크리드_로얄워터_9.jpg  \n",
            "   creating: 톰포드_화이트 스웨이드/\n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_1.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_10.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_12.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_13.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_14.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_15.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_16.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_18.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_2.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_20.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_21.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_22.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_24.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_26.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_27.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_28.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_3.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_30.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_32.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_33.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_34.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_35.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_36.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_4.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_41.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_42.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_43.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_44.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_47.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_48.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_49.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_5.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_51.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_53.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_54.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_56.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_60.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_7.jpg  \n",
            "  inflating: 톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_8.jpg  \n"
          ]
        }
      ],
      "source": [
        "# 향수 압축파일을 향수 폴더에 넣은 뒤 압축을 해제합니다.\n",
        "\n",
        "!unzip 향수_노중복.zip\n",
        "!rm 향수_노중복.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = os.listdir(path2)\n",
        "print(path2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0pEru2IqNTM",
        "outputId": "c419644b-fee2-4a76-c566-24da3cb8d885"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/향수\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlvRnA8fU5cr",
        "outputId": "742df8b7-99e5-42a5-cd15-e84a4a3261f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아쿠아디파르마_미르토', '딥디크_도손', '에르메스_운 자르뎅 수르닐', '크리드_로얄워터', '바이레도_집시워터', '르라보_상탈', '이솝_테싯', '톰포드_화이트 스웨이드', '마르지엘라_커피브레이크', '샤넬_블루드', '조말론_네롤리', '존바바토스_아티산']\n"
          ]
        }
      ],
      "source": [
        "print(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DXJFsS5-W8yr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#향수 이미지들을 image_datas 에 모두 넣습니다.\n",
        "\n",
        "image_datas = glob(f'{os.getcwd()}/*/*.jpg')\n",
        "class_name = folder_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubb5RspcW8oc",
        "outputId": "6b491148-59dc-4626-e823-540db5a04504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_35.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_12.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_50.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_28.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_10.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_60.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_31.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_11.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_41.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_4.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_26.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_37.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_43.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_9.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_18.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_24.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_30.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_2.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_46.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_3.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_33.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_45.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_44.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_6.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_19.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_1.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_40.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_27.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_15.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_49.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_29.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_38.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_48.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_57.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_13.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_17.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_59.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_39.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_53.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_47.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_25.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_32.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_56.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_51.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_20.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_5.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_34.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_54.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_7.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_42.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_36.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_16.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_58.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_23.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_22.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_8.jpg', '/content/향수/아쿠아디파르마_미르토/아쿠아디파르마_미르토_21.jpg', '/content/향수/딥디크_도손/딥디크_도손_31.jpg', '/content/향수/딥디크_도손/딥디크_도손_59.jpg', '/content/향수/딥디크_도손/딥디크_도손_38.jpg', '/content/향수/딥디크_도손/딥디크_도손_43.jpg', '/content/향수/딥디크_도손/딥디크_도손_50.jpg', '/content/향수/딥디크_도손/딥디크_도손_45.jpg', '/content/향수/딥디크_도손/딥디크_도손_17.jpg', '/content/향수/딥디크_도손/딥디크_도손_20.jpg', '/content/향수/딥디크_도손/딥디크_도손_49.jpg', '/content/향수/딥디크_도손/딥디크_도손_24.jpg', '/content/향수/딥디크_도손/딥디크_도손_9.jpg', '/content/향수/딥디크_도손/딥디크_도손_34.jpg', '/content/향수/딥디크_도손/딥디크_도손_46.jpg', '/content/향수/딥디크_도손/딥디크_도손_10.jpg', '/content/향수/딥디크_도손/딥디크_도손_16.jpg', '/content/향수/딥디크_도손/딥디크_도손_40.jpg', '/content/향수/딥디크_도손/딥디크_도손_6.jpg', '/content/향수/딥디크_도손/딥디크_도손_21.jpg', '/content/향수/딥디크_도손/딥디크_도손_36.jpg', '/content/향수/딥디크_도손/딥디크_도손_1.jpg', '/content/향수/딥디크_도손/딥디크_도손_55.jpg', '/content/향수/딥디크_도손/딥디크_도손_47.jpg', '/content/향수/딥디크_도손/딥디크_도손_5.jpg', '/content/향수/딥디크_도손/딥디크_도손_56.jpg', '/content/향수/딥디크_도손/딥디크_도손_37.jpg', '/content/향수/딥디크_도손/딥디크_도손_48.jpg', '/content/향수/딥디크_도손/딥디크_도손_54.jpg', '/content/향수/딥디크_도손/딥디크_도손_25.jpg', '/content/향수/딥디크_도손/딥디크_도손_8.jpg', '/content/향수/딥디크_도손/딥디크_도손_32.jpg', '/content/향수/딥디크_도손/딥디크_도손_13.jpg', '/content/향수/딥디크_도손/딥디크_도손_28.jpg', '/content/향수/딥디크_도손/딥디크_도손_12.jpg', '/content/향수/딥디크_도손/딥디크_도손_51.jpg', '/content/향수/딥디크_도손/딥디크_도손_42.jpg', '/content/향수/딥디크_도손/딥디크_도손_39.jpg', '/content/향수/딥디크_도손/딥디크_도손_4.jpg', '/content/향수/딥디크_도손/딥디크_도손_53.jpg', '/content/향수/딥디크_도손/딥디크_도손_2.jpg', '/content/향수/딥디크_도손/딥디크_도손_23.jpg', '/content/향수/딥디크_도손/딥디크_도손_19.jpg', '/content/향수/딥디크_도손/딥디크_도손_44.jpg', '/content/향수/딥디크_도손/딥디크_도손_15.jpg', '/content/향수/딥디크_도손/딥디크_도손_14.jpg', '/content/향수/딥디크_도손/딥디크_도손_41.jpg', '/content/향수/딥디크_도손/딥디크_도손_3.jpg', '/content/향수/딥디크_도손/딥디크_도손_35.jpg', '/content/향수/딥디크_도손/딥디크_도손_30.jpg', '/content/향수/딥디크_도손/딥디크_도손_27.jpg', '/content/향수/딥디크_도손/딥디크_도손_7.jpg', '/content/향수/딥디크_도손/딥디크_도손_33.jpg', '/content/향수/딥디크_도손/딥디크_도손_29.jpg', '/content/향수/딥디크_도손/딥디크_도손_58.jpg', '/content/향수/딥디크_도손/딥디크_도손_18.jpg', '/content/향수/딥디크_도손/딥디크_도손_52.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_36.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_9.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_33.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_45.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_59.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_27.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_34.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_24.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_10.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_49.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_12.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_20.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_37.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_8.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_60.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_29.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_58.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_6.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_17.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_3.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_53.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_31.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_15.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_25.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_18.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_44.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_48.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_28.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_32.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_40.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_41.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_5.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_43.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_4.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_42.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_13.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_11.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_7.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_16.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_1.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_52.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_35.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_50.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_46.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_21.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_2.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_19.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_22.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_30.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_39.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_47.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_56.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_14.jpg', '/content/향수/에르메스_운 자르뎅 수르닐/에르메스_운 자르뎅 수르닐_23.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_14.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_7.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_25.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_44.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_57.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_36.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_50.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_49.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_12.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_4.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_22.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_5.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_28.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_35.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_59.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_33.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_56.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_20.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_45.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_17.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_11.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_6.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_51.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_9.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_58.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_42.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_27.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_38.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_39.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_13.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_29.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_30.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_19.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_31.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_34.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_21.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_1.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_55.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_37.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_53.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_16.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_46.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_60.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_18.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_3.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_47.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_52.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_10.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_2.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_8.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_32.jpg', '/content/향수/크리드_로얄워터/크리드_로얄워터_15.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_32.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_59.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_55.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_18.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_16.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_48.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_43.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_26.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_6.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_19.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_60.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_31.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_37.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_41.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_3.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_22.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_57.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_7.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_23.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_58.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_9.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_29.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_38.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_20.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_11.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_34.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_21.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_14.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_1.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_5.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_25.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_51.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_54.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_30.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_53.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_46.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_10.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_44.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_52.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_56.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_8.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_12.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_47.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_17.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_27.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_4.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_35.jpg', '/content/향수/바이레도_집시워터/바이레도_집시워터_2.jpg', '/content/향수/르라보_상탈/르라보_상탈_39.jpg', '/content/향수/르라보_상탈/르라보_상탈_35.jpg', '/content/향수/르라보_상탈/르라보_상탈_21.jpg', '/content/향수/르라보_상탈/르라보_상탈_32.jpg', '/content/향수/르라보_상탈/르라보_상탈_30.jpg', '/content/향수/르라보_상탈/르라보_상탈_51.jpg', '/content/향수/르라보_상탈/르라보_상탈_52.jpg', '/content/향수/르라보_상탈/르라보_상탈_5.jpg', '/content/향수/르라보_상탈/르라보_상탈_19.jpg', '/content/향수/르라보_상탈/르라보_상탈_27.jpg', '/content/향수/르라보_상탈/르라보_상탈_18.jpg', '/content/향수/르라보_상탈/르라보_상탈_6.jpg', '/content/향수/르라보_상탈/르라보_상탈_26.jpg', '/content/향수/르라보_상탈/르라보_상탈_42.jpg', '/content/향수/르라보_상탈/르라보_상탈_45.jpg', '/content/향수/르라보_상탈/르라보_상탈_40.jpg', '/content/향수/르라보_상탈/르라보_상탈_33.jpg', '/content/향수/르라보_상탈/르라보_상탈_53.jpg', '/content/향수/르라보_상탈/르라보_상탈_17.jpg', '/content/향수/르라보_상탈/르라보_상탈_31.jpg', '/content/향수/르라보_상탈/르라보_상탈_8.jpg', '/content/향수/르라보_상탈/르라보_상탈_4.jpg', '/content/향수/르라보_상탈/르라보_상탈_54.jpg', '/content/향수/르라보_상탈/르라보_상탈_56.jpg', '/content/향수/르라보_상탈/르라보_상탈_10.jpg', '/content/향수/르라보_상탈/르라보_상탈_12.jpg', '/content/향수/르라보_상탈/르라보_상탈_13.jpg', '/content/향수/르라보_상탈/르라보_상탈_38.jpg', '/content/향수/르라보_상탈/르라보_상탈_28.jpg', '/content/향수/르라보_상탈/르라보_상탈_60.jpg', '/content/향수/르라보_상탈/르라보_상탈_7.jpg', '/content/향수/르라보_상탈/르라보_상탈_25.jpg', '/content/향수/르라보_상탈/르라보_상탈_57.jpg', '/content/향수/르라보_상탈/르라보_상탈_2.jpg', '/content/향수/르라보_상탈/르라보_상탈_23.jpg', '/content/향수/르라보_상탈/르라보_상탈_15.jpg', '/content/향수/르라보_상탈/르라보_상탈_48.jpg', '/content/향수/르라보_상탈/르라보_상탈_24.jpg', '/content/향수/르라보_상탈/르라보_상탈_16.jpg', '/content/향수/르라보_상탈/르라보_상탈_47.jpg', '/content/향수/르라보_상탈/르라보_상탈_14.jpg', '/content/향수/르라보_상탈/르라보_상탈_37.jpg', '/content/향수/르라보_상탈/르라보_상탈_46.jpg', '/content/향수/르라보_상탈/르라보_상탈_36.jpg', '/content/향수/르라보_상탈/르라보_상탈_1.jpg', '/content/향수/르라보_상탈/르라보_상탈_11.jpg', '/content/향수/르라보_상탈/르라보_상탈_20.jpg', '/content/향수/르라보_상탈/르라보_상탈_3.jpg', '/content/향수/르라보_상탈/르라보_상탈_58.jpg', '/content/향수/르라보_상탈/르라보_상탈_9.jpg', '/content/향수/르라보_상탈/르라보_상탈_29.jpg', '/content/향수/이솝_테싯/이솝_테싯_10.jpg', '/content/향수/이솝_테싯/이솝_테싯_43.jpg', '/content/향수/이솝_테싯/이솝_테싯_5.jpg', '/content/향수/이솝_테싯/이솝_테싯_7.jpg', '/content/향수/이솝_테싯/이솝_테싯_38.jpg', '/content/향수/이솝_테싯/이솝_테싯_20.jpg', '/content/향수/이솝_테싯/이솝_테싯_45.jpg', '/content/향수/이솝_테싯/이솝_테싯_32.jpg', '/content/향수/이솝_테싯/이솝_테싯_28.jpg', '/content/향수/이솝_테싯/이솝_테싯_44.jpg', '/content/향수/이솝_테싯/이솝_테싯_58.jpg', '/content/향수/이솝_테싯/이솝_테싯_9.jpg', '/content/향수/이솝_테싯/이솝_테싯_46.jpg', '/content/향수/이솝_테싯/이솝_테싯_23.jpg', '/content/향수/이솝_테싯/이솝_테싯_1.jpg', '/content/향수/이솝_테싯/이솝_테싯_36.jpg', '/content/향수/이솝_테싯/이솝_테싯_3.jpg', '/content/향수/이솝_테싯/이솝_테싯_29.jpg', '/content/향수/이솝_테싯/이솝_테싯_49.jpg', '/content/향수/이솝_테싯/이솝_테싯_48.jpg', '/content/향수/이솝_테싯/이솝_테싯_56.jpg', '/content/향수/이솝_테싯/이솝_테싯_22.jpg', '/content/향수/이솝_테싯/이솝_테싯_21.jpg', '/content/향수/이솝_테싯/이솝_테싯_54.jpg', '/content/향수/이솝_테싯/이솝_테싯_52.jpg', '/content/향수/이솝_테싯/이솝_테싯_50.jpg', '/content/향수/이솝_테싯/이솝_테싯_33.jpg', '/content/향수/이솝_테싯/이솝_테싯_26.jpg', '/content/향수/이솝_테싯/이솝_테싯_27.jpg', '/content/향수/이솝_테싯/이솝_테싯_55.jpg', '/content/향수/이솝_테싯/이솝_테싯_12.jpg', '/content/향수/이솝_테싯/이솝_테싯_11.jpg', '/content/향수/이솝_테싯/이솝_테싯_4.jpg', '/content/향수/이솝_테싯/이솝_테싯_14.jpg', '/content/향수/이솝_테싯/이솝_테싯_40.jpg', '/content/향수/이솝_테싯/이솝_테싯_31.jpg', '/content/향수/이솝_테싯/이솝_테싯_24.jpg', '/content/향수/이솝_테싯/이솝_테싯_34.jpg', '/content/향수/이솝_테싯/이솝_테싯_6.jpg', '/content/향수/이솝_테싯/이솝_테싯_18.jpg', '/content/향수/이솝_테싯/이솝_테싯_16.jpg', '/content/향수/이솝_테싯/이솝_테싯_30.jpg', '/content/향수/이솝_테싯/이솝_테싯_41.jpg', '/content/향수/이솝_테싯/이솝_테싯_39.jpg', '/content/향수/이솝_테싯/이솝_테싯_13.jpg', '/content/향수/이솝_테싯/이솝_테싯_2.jpg', '/content/향수/이솝_테싯/이솝_테싯_37.jpg', '/content/향수/이솝_테싯/이솝_테싯_35.jpg', '/content/향수/이솝_테싯/이솝_테싯_25.jpg', '/content/향수/이솝_테싯/이솝_테싯_59.jpg', '/content/향수/이솝_테싯/이솝_테싯_57.jpg', '/content/향수/이솝_테싯/이솝_테싯_60.jpg', '/content/향수/이솝_테싯/이솝_테싯_42.jpg', '/content/향수/이솝_테싯/이솝_테싯_17.jpg', '/content/향수/이솝_테싯/이솝_테싯_8.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_49.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_14.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_21.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_18.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_33.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_51.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_41.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_13.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_42.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_60.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_4.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_32.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_12.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_26.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_36.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_20.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_53.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_56.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_2.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_3.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_22.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_24.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_1.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_27.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_44.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_54.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_47.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_8.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_16.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_5.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_43.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_34.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_48.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_10.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_28.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_35.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_30.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_7.jpg', '/content/향수/톰포드_화이트 스웨이드/톰포드_화이트 스웨이드_15.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_10.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_15.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_34.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_21.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_8.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_58.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_57.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_53.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_49.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_40.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_42.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_44.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_22.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_16.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_17.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_51.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_19.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_56.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_5.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_33.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_18.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_9.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_60.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_50.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_39.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_11.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_4.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_35.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_29.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_2.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_55.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_3.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_30.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_26.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_20.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_31.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_6.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_41.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_23.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_7.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_32.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_47.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_14.jpg', '/content/향수/마르지엘라_커피브레이크/마르지엘라_커피브레이크_1.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_18.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_8.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_36.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_56.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_37.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_59.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_47.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_34.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_10.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_46.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_51.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_2.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_33.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_43.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_9.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_53.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_1.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_32.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_30.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_6.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_4.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_12.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_50.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_58.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_26.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_25.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_41.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_5.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_24.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_42.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_44.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_31.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_49.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_20.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_45.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_3.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_19.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_57.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_29.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_39.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_21.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_11.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_35.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_16.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_17.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_13.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_15.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_48.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_54.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_7.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_22.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_60.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_28.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_40.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_14.jpg', '/content/향수/샤넬_블루드/샤넬_블루드_38.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_33.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_40.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_44.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_43.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_54.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_21.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_45.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_46.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_12.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_2.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_48.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_8.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_19.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_22.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_3.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_7.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_13.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_14.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_31.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_27.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_42.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_56.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_32.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_58.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_41.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_4.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_20.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_30.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_25.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_60.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_26.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_1.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_9.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_36.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_47.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_16.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_15.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_38.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_10.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_53.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_28.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_23.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_52.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_51.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_29.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_49.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_6.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_5.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_39.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_50.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_57.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_55.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_24.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_35.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_34.jpg', '/content/향수/조말론_네롤리/조말론_네롤리_17.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_10.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_32.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_26.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_7.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_21.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_24.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_8.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_52.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_13.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_27.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_49.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_23.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_51.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_44.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_28.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_54.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_25.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_29.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_45.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_56.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_35.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_12.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_2.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_50.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_9.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_14.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_36.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_19.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_15.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_1.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_18.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_47.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_55.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_3.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_57.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_42.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_41.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_46.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_4.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_20.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_39.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_37.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_43.jpg', '/content/향수/존바바토스_아티산/존바바토스_아티산_40.jpg']\n",
            "611\n"
          ]
        }
      ],
      "source": [
        "print(image_datas)\n",
        "\n",
        "print(len(image_datas)) #총 이미지 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fx0sSC67ZO2H"
      },
      "outputs": [],
      "source": [
        "#향수 이미지 파일을 열어서 변환합니다.\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "for imagename in image_datas:\n",
        "    image = Image.open(imagename).convert('RGB')\n",
        "    image = image.resize((224, 224))\n",
        "    image = np.array(image)\n",
        "    X.append(image)\n",
        "    label = imagename.split('/')[3]\n",
        "    Y.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "pu6GOqcXdy8a",
        "outputId": "6031aef2-0c9a-47a4-9482-48124b730bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "611\n",
            "아쿠아디파르마_미르토\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#첫번째 향수 보기\n",
        "\n",
        "plt.imshow(X[0])\n",
        "print(len(X))\n",
        "print(Y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoQDoQ01boud",
        "outputId": "64ea7c49-a289-47a8-bab7-b0097f511616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(611, 224, 224, 3)\n",
            "(611,)\n"
          ]
        }
      ],
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "train_images = X.astype('float32') / 255 #일반화\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "OEMqWBrAqiIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240f11ab-ce52-4066-ee93-867bbb99786d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_45 (Conv2D)          (None, 222, 222, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 220, 220, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_35 (MaxPoolin  (None, 110, 110, 64)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 108, 108, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_48 (Conv2D)          (None, 106, 106, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_36 (MaxPoolin  (None, 53, 53, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 51, 51, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 49, 49, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_37 (MaxPoolin  (None, 24, 24, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 147456)            0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 12)                1769484   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,914,892\n",
            "Trainable params: 2,914,892\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#첫번째 모델\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(224,224,3)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Flatten()) \n",
        "model.add(layers.Dense(len(folder_name),activation=\"softmax\"))\n",
        "\n",
        "model.summary()\n",
        "    \n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjov4jG4yx4P",
        "outputId": "1e6e9a12-5b1d-4287-8235-718aa93e3d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57 55 54 52 48 51 55 39 44 56 56 44 \n",
            "611\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ]
        }
      ],
      "source": [
        "#라벨을 숫자로 변환\n",
        "\n",
        "train_labels = [0]\n",
        "\n",
        "count = 0\n",
        "for i in range(1, len(Y)):\n",
        "  if Y[i] == Y[i-1]:\n",
        "    train_labels.append(count)\n",
        "  else:\n",
        "    count += 1\n",
        "    train_labels.append(count)\n",
        "  \n",
        "for i in range(len(folder_name)):\n",
        "  print(train_labels.count(i), end = \" \")\n",
        "print()\n",
        "\n",
        "print(len(train_labels))\n",
        "print(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQnvK4oK1qNz",
        "outputId": "7d31a7e0-d369-42a6-f45d-698d7c900705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "611\n"
          ]
        }
      ],
      "source": [
        "#ont-hot 인코딩\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "print(train_labels)\n",
        "print(len(train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(선택1) 테스트 데이터 나누기\n",
        "# 테스트 데이터셋을 20% 햘당함\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, train_labels, test_size=0.2, shuffle=True, stratify=train_labels, random_state=34)\n",
        "\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255"
      ],
      "metadata": {
        "id": "3uPyj1lM3Vfa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpRqVIUO3mJV",
        "outputId": "c3169f2f-e9dc-4ffd-b77f-3c77dedd4abe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uY_sS6ct8Og",
        "outputId": "eade6aff-0905-4aa4-df18-ef78f52ad925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 43s 388ms/step - loss: 2.6289 - accuracy: 0.1086\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 2.2818 - accuracy: 0.1824\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 2.1955 - accuracy: 0.2152\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 2.1159 - accuracy: 0.2029\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.9868 - accuracy: 0.2930\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 1.8019 - accuracy: 0.3893\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 1.7824 - accuracy: 0.3873\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 1.7026 - accuracy: 0.4119\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.8657 - accuracy: 0.3586\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.6314 - accuracy: 0.4324\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.6650 - accuracy: 0.4406\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.5950 - accuracy: 0.4570\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.5527 - accuracy: 0.4508\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.4884 - accuracy: 0.4754\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 6s 411ms/step - loss: 1.5428 - accuracy: 0.4713\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.5243 - accuracy: 0.4836\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.4794 - accuracy: 0.4672\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.4365 - accuracy: 0.5020\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 1.4611 - accuracy: 0.5102\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3723 - accuracy: 0.5348\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.4201 - accuracy: 0.5082\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.4532 - accuracy: 0.5102\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.3987 - accuracy: 0.4980\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.3357 - accuracy: 0.5430\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.3815 - accuracy: 0.5225\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.2833 - accuracy: 0.5676\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 1.2959 - accuracy: 0.5574\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.3735 - accuracy: 0.5410\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.5537 - accuracy: 0.5205\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.3608 - accuracy: 0.5307\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.3256 - accuracy: 0.5492\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 1.2857 - accuracy: 0.5451\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.2577 - accuracy: 0.5328\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.2669 - accuracy: 0.5676\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.1865 - accuracy: 0.5656\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 1.2867 - accuracy: 0.5451\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.2283 - accuracy: 0.5758\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.2503 - accuracy: 0.5533\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.1249 - accuracy: 0.6189\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.2768 - accuracy: 0.5533\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.2032 - accuracy: 0.5676\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.2467 - accuracy: 0.5574\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.1992 - accuracy: 0.5697\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.0585 - accuracy: 0.6270\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0879 - accuracy: 0.6270\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 1.1469 - accuracy: 0.6004\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 1.0622 - accuracy: 0.6291\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.1250 - accuracy: 0.6107\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0572 - accuracy: 0.6148\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.1308 - accuracy: 0.6127\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.0441 - accuracy: 0.6598\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 1.0173 - accuracy: 0.6455\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.0143 - accuracy: 0.6516\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.0489 - accuracy: 0.6414\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 1.0574 - accuracy: 0.6598\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.9740 - accuracy: 0.6598\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.0241 - accuracy: 0.6537\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.9302 - accuracy: 0.6926\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.0383 - accuracy: 0.6516\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 1.0283 - accuracy: 0.6332\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.9086 - accuracy: 0.7070\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.9608 - accuracy: 0.6578\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.9078 - accuracy: 0.6824\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.8265 - accuracy: 0.6906\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.8799 - accuracy: 0.7008\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.8806 - accuracy: 0.7131\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.9327 - accuracy: 0.6680\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.8994 - accuracy: 0.6762\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.8768 - accuracy: 0.6824\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.7840 - accuracy: 0.7172\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.7379 - accuracy: 0.7316\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.7943 - accuracy: 0.7090\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.8169 - accuracy: 0.7131\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.8679 - accuracy: 0.7254\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.7157 - accuracy: 0.7705\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 6s 409ms/step - loss: 0.8404 - accuracy: 0.7193\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.7491 - accuracy: 0.7541\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.7455 - accuracy: 0.7439\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 6s 412ms/step - loss: 0.7438 - accuracy: 0.7561\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.6877 - accuracy: 0.7582\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.7583 - accuracy: 0.7275\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.8208 - accuracy: 0.7439\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.6589 - accuracy: 0.7643\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 1.1172 - accuracy: 0.7254\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.7847 - accuracy: 0.7520\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.6942 - accuracy: 0.7787\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.6901 - accuracy: 0.7725\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5873 - accuracy: 0.7848\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.6364 - accuracy: 0.7930\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.7909 - accuracy: 0.7520\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.6152 - accuracy: 0.8135\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.6055 - accuracy: 0.8115\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.6010 - accuracy: 0.7848\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5649 - accuracy: 0.8012\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5935 - accuracy: 0.8053\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5983 - accuracy: 0.8197\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.6012 - accuracy: 0.7869\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.6543 - accuracy: 0.8012\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5726 - accuracy: 0.8115\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5138 - accuracy: 0.8422\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5239 - accuracy: 0.8238\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5187 - accuracy: 0.8340\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.4134 - accuracy: 0.8791\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5651 - accuracy: 0.8135\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.3860 - accuracy: 0.8730\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.3962 - accuracy: 0.8607\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 0.4445 - accuracy: 0.8443\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.6879 - accuracy: 0.7910\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.4605 - accuracy: 0.8402\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 6s 407ms/step - loss: 0.3413 - accuracy: 0.8791\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 6s 407ms/step - loss: 0.4815 - accuracy: 0.8443\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.4580 - accuracy: 0.8525\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.4199 - accuracy: 0.8689\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.4324 - accuracy: 0.8668\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.4621 - accuracy: 0.8566\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.4752 - accuracy: 0.8586\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5192 - accuracy: 0.8402\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.4190 - accuracy: 0.8607\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3926 - accuracy: 0.8893\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.3478 - accuracy: 0.9037\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.3832 - accuracy: 0.8750\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.3096 - accuracy: 0.8934\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.2607 - accuracy: 0.9016\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.3283 - accuracy: 0.8873\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.3563 - accuracy: 0.8770\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 0.4866 - accuracy: 0.8504\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.4955 - accuracy: 0.8525\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 0.3425 - accuracy: 0.8873\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.2678 - accuracy: 0.9201\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.3601 - accuracy: 0.8893\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3491 - accuracy: 0.8934\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.3879 - accuracy: 0.8873\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.2715 - accuracy: 0.8996\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.2683 - accuracy: 0.9119\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.2751 - accuracy: 0.9139\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3471 - accuracy: 0.8996\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.3556 - accuracy: 0.8709\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.3324 - accuracy: 0.9098\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3980 - accuracy: 0.8709\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.2827 - accuracy: 0.9180\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.2810 - accuracy: 0.9180\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.2672 - accuracy: 0.9139\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.3183 - accuracy: 0.8852\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3078 - accuracy: 0.8873\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3144 - accuracy: 0.8996\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.3596 - accuracy: 0.8934\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.3116 - accuracy: 0.9078\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.2309 - accuracy: 0.9262\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.2287 - accuracy: 0.9303\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.2415 - accuracy: 0.8975\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f169608be50>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "#모델 돌리기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model.fit(train_gen, epochs=150)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9fKnxyneUNr",
        "outputId": "5c3b5a88-9092-4eb1-a1a7-ea4720db20f4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 188ms/step - loss: 2.8251 - accuracy: 0.7073\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.8251051902770996, 0.707317054271698]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 모델 한줄빠진거\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model2 = models.Sequential()\n",
        "\n",
        "model2.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(224,224,3)))\n",
        "model2.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(layers.Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(layers.Flatten()) \n",
        "model2.add(layers.Dense(len(folder_name),activation=\"softmax\"))\n",
        "\n",
        "model2.summary()\n",
        "    \n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF9jUdvOMtJ6",
        "outputId": "8a7e1460-b9a3-49e6-c60f-1e15016e27ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_32 (Conv2D)          (None, 222, 222, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 220, 220, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 110, 110, 64)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 108, 108, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 106, 106, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 53, 53, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 51, 51, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  (None, 25, 25, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,475,340\n",
            "Trainable params: 2,475,340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 돌리기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model2.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgUIia7eM2US",
        "outputId": "89b021b9-1a11-47ac-c418-b8deafad042c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 7s 367ms/step - loss: 2.6242 - accuracy: 0.1168\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 2.3492 - accuracy: 0.1701\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 2.2953 - accuracy: 0.2070\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 2.2607 - accuracy: 0.2234\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 2.0555 - accuracy: 0.2889\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 2.0375 - accuracy: 0.2623\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.9242 - accuracy: 0.3381\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.8601 - accuracy: 0.3525\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.8834 - accuracy: 0.3545\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.8451 - accuracy: 0.3586\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 1.7809 - accuracy: 0.3750\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.6967 - accuracy: 0.4180\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.7066 - accuracy: 0.3955\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 1.5713 - accuracy: 0.4652\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.5563 - accuracy: 0.4898\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 7s 402ms/step - loss: 1.5166 - accuracy: 0.4795\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.4156 - accuracy: 0.5041\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.4685 - accuracy: 0.4939\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 1.4210 - accuracy: 0.5102\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.3381 - accuracy: 0.5389\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.3223 - accuracy: 0.5533\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 1.2667 - accuracy: 0.5574\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.2585 - accuracy: 0.5779\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.2832 - accuracy: 0.5492\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.1370 - accuracy: 0.6209\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.1327 - accuracy: 0.6025\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.2336 - accuracy: 0.5984\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.2849 - accuracy: 0.6066\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.2642 - accuracy: 0.5758\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 7s 425ms/step - loss: 1.2755 - accuracy: 0.5676\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.2610 - accuracy: 0.5717\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 1.1340 - accuracy: 0.6209\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.2347 - accuracy: 0.5635\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.1531 - accuracy: 0.6066\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 1.1790 - accuracy: 0.5922\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 1.0621 - accuracy: 0.6270\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.0399 - accuracy: 0.6455\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.0320 - accuracy: 0.6168\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.9377 - accuracy: 0.6660\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.0232 - accuracy: 0.6557\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.9252 - accuracy: 0.6824\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.8677 - accuracy: 0.7008\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8794 - accuracy: 0.6906\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8380 - accuracy: 0.7234\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.8336 - accuracy: 0.6967\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.0010 - accuracy: 0.6762\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.9788 - accuracy: 0.6660\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.8517 - accuracy: 0.7193\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.7912 - accuracy: 0.7275\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8368 - accuracy: 0.7316\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7867 - accuracy: 0.7459\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.7557 - accuracy: 0.7561\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.6756 - accuracy: 0.7664\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.7264 - accuracy: 0.7602\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.7357 - accuracy: 0.7275\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.7497 - accuracy: 0.7541\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8979 - accuracy: 0.7131\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.1864 - accuracy: 0.6168\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.9918 - accuracy: 0.6578\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8358 - accuracy: 0.7316\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.8581 - accuracy: 0.7008\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.8449 - accuracy: 0.7131\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 1.0015 - accuracy: 0.6619\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.9126 - accuracy: 0.7008\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8634 - accuracy: 0.7213\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.8301 - accuracy: 0.7377\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.7399 - accuracy: 0.7684\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.7256 - accuracy: 0.7480\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.8300 - accuracy: 0.7234\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.7494 - accuracy: 0.7480\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6901 - accuracy: 0.7910\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6669 - accuracy: 0.7848\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.6364 - accuracy: 0.7930\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5887 - accuracy: 0.8053\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5391 - accuracy: 0.8340\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5344 - accuracy: 0.8217\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4656 - accuracy: 0.8381\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.6899 - accuracy: 0.7848\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8296 - accuracy: 0.7520\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6820 - accuracy: 0.8074\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5688 - accuracy: 0.8115\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5405 - accuracy: 0.8340\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6847 - accuracy: 0.7992\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5134 - accuracy: 0.8176\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5035 - accuracy: 0.8484\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4951 - accuracy: 0.8709\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6488 - accuracy: 0.7910\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6450 - accuracy: 0.7930\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5541 - accuracy: 0.8217\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.5859 - accuracy: 0.8033\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5108 - accuracy: 0.8320\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5302 - accuracy: 0.8135\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4703 - accuracy: 0.8443\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4643 - accuracy: 0.8361\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.4616 - accuracy: 0.8320\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4520 - accuracy: 0.8443\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.7106 - accuracy: 0.7992\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4427 - accuracy: 0.8443\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.3796 - accuracy: 0.8709\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3441 - accuracy: 0.8975\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.4154 - accuracy: 0.8627\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.4367 - accuracy: 0.8361\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3496 - accuracy: 0.8832\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4165 - accuracy: 0.8422\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3656 - accuracy: 0.8791\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.3725 - accuracy: 0.9016\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3077 - accuracy: 0.8975\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3815 - accuracy: 0.8607\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4035 - accuracy: 0.8648\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.3976 - accuracy: 0.8770\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.3628 - accuracy: 0.8791\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.3732 - accuracy: 0.8750\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.3266 - accuracy: 0.9098\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.3304 - accuracy: 0.8996\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3421 - accuracy: 0.8975\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.3925 - accuracy: 0.8525\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.3988 - accuracy: 0.8750\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.2939 - accuracy: 0.9016\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3683 - accuracy: 0.9016\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.3820 - accuracy: 0.8975\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.2741 - accuracy: 0.9098\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.2328 - accuracy: 0.9160\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3237 - accuracy: 0.8934\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3425 - accuracy: 0.8873\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.2837 - accuracy: 0.9078\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.8443 - accuracy: 0.8094\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6136 - accuracy: 0.8197\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3652 - accuracy: 0.8934\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3349 - accuracy: 0.9139\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.2944 - accuracy: 0.8996\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.2818 - accuracy: 0.9078\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.3209 - accuracy: 0.8852\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.2994 - accuracy: 0.9180\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.3260 - accuracy: 0.9139\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.2895 - accuracy: 0.9057\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.2253 - accuracy: 0.9283\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.2060 - accuracy: 0.9262\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.2726 - accuracy: 0.9037\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.2438 - accuracy: 0.9385\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.1998 - accuracy: 0.9344\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.2062 - accuracy: 0.9406\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.2921 - accuracy: 0.9119\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.2496 - accuracy: 0.9139\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.2272 - accuracy: 0.9344\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.1830 - accuracy: 0.9365\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.2309 - accuracy: 0.9365\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.2239 - accuracy: 0.9324\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.2753 - accuracy: 0.9221\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.2496 - accuracy: 0.9057\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.2627 - accuracy: 0.9180\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d4c117790>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRimjzWo4blp",
        "outputId": "316eadbb-34c8-4f61-895e-ae5df40e7a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 95ms/step - loss: 3.2953 - accuracy: 0.6585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.295325756072998, 0.6585366129875183]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기존 모델로 돌리기(위에서 기존 모델이 정확도가 더 높았기 때문)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model2.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v9OtmiARAPi",
        "outputId": "c5ac628c-f91b-484a-ab94-920a18fa6005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 7s 372ms/step - loss: 2.5983 - accuracy: 0.1107\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 2.2693 - accuracy: 0.1578\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 2.1856 - accuracy: 0.2172\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 2.0789 - accuracy: 0.2766\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.9732 - accuracy: 0.2951\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 2.0229 - accuracy: 0.2889\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.9941 - accuracy: 0.2889\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.9293 - accuracy: 0.3299\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.8565 - accuracy: 0.3443\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.8665 - accuracy: 0.3627\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.8349 - accuracy: 0.3689\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.7838 - accuracy: 0.3914\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.7378 - accuracy: 0.4221\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.7040 - accuracy: 0.4242\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.6481 - accuracy: 0.4078\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 6s 396ms/step - loss: 1.6342 - accuracy: 0.4426\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.6842 - accuracy: 0.4221\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 1.5472 - accuracy: 0.4467\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.6321 - accuracy: 0.4078\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.6153 - accuracy: 0.4201\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 6s 398ms/step - loss: 1.6329 - accuracy: 0.4098\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.5405 - accuracy: 0.4672\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.5789 - accuracy: 0.4713\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.4733 - accuracy: 0.4816\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.4786 - accuracy: 0.4590\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4448 - accuracy: 0.4877\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.3623 - accuracy: 0.4939\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.3178 - accuracy: 0.5676\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4100 - accuracy: 0.5082\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.5177 - accuracy: 0.4590\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.4474 - accuracy: 0.5246\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.3803 - accuracy: 0.5328\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.3389 - accuracy: 0.5205\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.4462 - accuracy: 0.5000\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.4895 - accuracy: 0.5123\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.3958 - accuracy: 0.5328\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.3823 - accuracy: 0.5041\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.3561 - accuracy: 0.5594\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.2733 - accuracy: 0.5430\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.2184 - accuracy: 0.5635\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.2431 - accuracy: 0.5533\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.3461 - accuracy: 0.5471\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 1.1639 - accuracy: 0.5820\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.2568 - accuracy: 0.5635\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.3408 - accuracy: 0.5348\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.2109 - accuracy: 0.5799\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.2098 - accuracy: 0.6004\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 1.2735 - accuracy: 0.5717\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.2040 - accuracy: 0.5656\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.3754 - accuracy: 0.5635\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.2000 - accuracy: 0.5840\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.1222 - accuracy: 0.6025\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.0951 - accuracy: 0.6250\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.0621 - accuracy: 0.6537\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.0769 - accuracy: 0.6270\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.9955 - accuracy: 0.6537\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.0316 - accuracy: 0.6516\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.0786 - accuracy: 0.6455\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.9660 - accuracy: 0.6967\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.9128 - accuracy: 0.6947\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.9699 - accuracy: 0.6926\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.8453 - accuracy: 0.7008\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.9316 - accuracy: 0.6762\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.8603 - accuracy: 0.6865\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.8496 - accuracy: 0.7131\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.7780 - accuracy: 0.7398\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.9182 - accuracy: 0.6865\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.8008 - accuracy: 0.7398\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.8192 - accuracy: 0.7439\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.8777 - accuracy: 0.6926\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.8203 - accuracy: 0.7213\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.7660 - accuracy: 0.7705\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7156 - accuracy: 0.7684\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6920 - accuracy: 0.7684\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.9010 - accuracy: 0.7193\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.7876 - accuracy: 0.7541\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.0032 - accuracy: 0.6885\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.8512 - accuracy: 0.7500\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7060 - accuracy: 0.7643\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7615 - accuracy: 0.7561\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.7893 - accuracy: 0.7623\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.7273 - accuracy: 0.7480\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.7643 - accuracy: 0.7561\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.7007 - accuracy: 0.7848\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.7303 - accuracy: 0.7664\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6927 - accuracy: 0.7766\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6189 - accuracy: 0.7787\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 6s 396ms/step - loss: 0.7120 - accuracy: 0.7766\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 6s 398ms/step - loss: 0.6887 - accuracy: 0.7828\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.6307 - accuracy: 0.7889\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6451 - accuracy: 0.7951\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6367 - accuracy: 0.8094\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6758 - accuracy: 0.8156\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.7871 - accuracy: 0.7357\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6381 - accuracy: 0.7869\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6468 - accuracy: 0.7951\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5436 - accuracy: 0.8238\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5823 - accuracy: 0.8094\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6070 - accuracy: 0.7951\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6070 - accuracy: 0.8197\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5549 - accuracy: 0.8135\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6106 - accuracy: 0.8012\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5540 - accuracy: 0.8156\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5667 - accuracy: 0.8176\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6495 - accuracy: 0.7992\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5759 - accuracy: 0.8094\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5906 - accuracy: 0.8033\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6191 - accuracy: 0.8115\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5420 - accuracy: 0.8299\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5153 - accuracy: 0.8217\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4763 - accuracy: 0.8504\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5252 - accuracy: 0.8463\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5770 - accuracy: 0.8238\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5047 - accuracy: 0.8422\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5081 - accuracy: 0.8340\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5299 - accuracy: 0.8299\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4473 - accuracy: 0.8566\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 0.4933 - accuracy: 0.8402\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4650 - accuracy: 0.8668\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4948 - accuracy: 0.8320\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4630 - accuracy: 0.8463\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4535 - accuracy: 0.8463\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5623 - accuracy: 0.8197\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.4888 - accuracy: 0.8197\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4630 - accuracy: 0.8422\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5701 - accuracy: 0.8135\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4695 - accuracy: 0.8504\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4505 - accuracy: 0.8566\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.4054 - accuracy: 0.8730\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5354 - accuracy: 0.8361\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5309 - accuracy: 0.8484\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5379 - accuracy: 0.8238\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5064 - accuracy: 0.8463\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4151 - accuracy: 0.8648\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.3649 - accuracy: 0.8852\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4642 - accuracy: 0.8566\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3909 - accuracy: 0.8730\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3642 - accuracy: 0.8689\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3484 - accuracy: 0.8955\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.3704 - accuracy: 0.8770\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4381 - accuracy: 0.8586\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.3209 - accuracy: 0.8832\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4851 - accuracy: 0.8627\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4501 - accuracy: 0.8525\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6835 - accuracy: 0.8135\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5369 - accuracy: 0.8340\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4434 - accuracy: 0.8586\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4010 - accuracy: 0.8730\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.3819 - accuracy: 0.8689\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.4899 - accuracy: 0.8484\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d3786d690>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwbok8B4RIEV",
        "outputId": "d5bb975b-14d4-4c17-8a28-b973df44f676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 98ms/step - loss: 2.7452 - accuracy: 0.6829\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7451887130737305, 0.6829268336296082]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기존 모델로 돌리기(위에서 기존 모델이 정확도가 더 높았기 때문)\n",
        "# 에폭을 200으로 늘려보기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model2.fit(train_gen, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG3VA23MViAs",
        "outputId": "232e9681-f689-4c93-f003-ff170682cc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "16/16 [==============================] - 7s 371ms/step - loss: 3.0801 - accuracy: 0.0861\n",
            "Epoch 2/200\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 2.3346 - accuracy: 0.1926\n",
            "Epoch 3/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 2.2661 - accuracy: 0.2193\n",
            "Epoch 4/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 2.1170 - accuracy: 0.2602\n",
            "Epoch 5/200\n",
            "16/16 [==============================] - 6s 398ms/step - loss: 2.0301 - accuracy: 0.3135\n",
            "Epoch 6/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.9133 - accuracy: 0.3422\n",
            "Epoch 7/200\n",
            "16/16 [==============================] - 7s 426ms/step - loss: 1.9473 - accuracy: 0.3340\n",
            "Epoch 8/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.9974 - accuracy: 0.2725\n",
            "Epoch 9/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.9038 - accuracy: 0.3299\n",
            "Epoch 10/200\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.7616 - accuracy: 0.3648\n",
            "Epoch 11/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.7601 - accuracy: 0.4180\n",
            "Epoch 12/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.7837 - accuracy: 0.3996\n",
            "Epoch 13/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.7073 - accuracy: 0.3955\n",
            "Epoch 14/200\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 1.6947 - accuracy: 0.4057\n",
            "Epoch 15/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.6754 - accuracy: 0.4221\n",
            "Epoch 16/200\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 1.6665 - accuracy: 0.4488\n",
            "Epoch 17/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.7141 - accuracy: 0.4488\n",
            "Epoch 18/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.5319 - accuracy: 0.4775\n",
            "Epoch 19/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.5695 - accuracy: 0.4488\n",
            "Epoch 20/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.5549 - accuracy: 0.4570\n",
            "Epoch 21/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.5506 - accuracy: 0.4549\n",
            "Epoch 22/200\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 1.5291 - accuracy: 0.4980\n",
            "Epoch 23/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.6193 - accuracy: 0.4406\n",
            "Epoch 24/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4839 - accuracy: 0.4754\n",
            "Epoch 25/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.4774 - accuracy: 0.4980\n",
            "Epoch 26/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.5959 - accuracy: 0.4365\n",
            "Epoch 27/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.5127 - accuracy: 0.4713\n",
            "Epoch 28/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.4875 - accuracy: 0.4508\n",
            "Epoch 29/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.5838 - accuracy: 0.4570\n",
            "Epoch 30/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.4403 - accuracy: 0.4734\n",
            "Epoch 31/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.5413 - accuracy: 0.4877\n",
            "Epoch 32/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4772 - accuracy: 0.4775\n",
            "Epoch 33/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.5374 - accuracy: 0.4611\n",
            "Epoch 34/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.4187 - accuracy: 0.5082\n",
            "Epoch 35/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4984 - accuracy: 0.4549\n",
            "Epoch 36/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.5048 - accuracy: 0.5123\n",
            "Epoch 37/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4611 - accuracy: 0.4734\n",
            "Epoch 38/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4880 - accuracy: 0.4898\n",
            "Epoch 39/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.4386 - accuracy: 0.5184\n",
            "Epoch 40/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.3437 - accuracy: 0.5553\n",
            "Epoch 41/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.5056 - accuracy: 0.4877\n",
            "Epoch 42/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.2680 - accuracy: 0.5697\n",
            "Epoch 43/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.2769 - accuracy: 0.5533\n",
            "Epoch 44/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.3358 - accuracy: 0.5328\n",
            "Epoch 45/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.3550 - accuracy: 0.5369\n",
            "Epoch 46/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.3108 - accuracy: 0.5328\n",
            "Epoch 47/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.2739 - accuracy: 0.5902\n",
            "Epoch 48/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.1890 - accuracy: 0.5922\n",
            "Epoch 49/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2009 - accuracy: 0.5902\n",
            "Epoch 50/200\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.1631 - accuracy: 0.5881\n",
            "Epoch 51/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.2159 - accuracy: 0.5717\n",
            "Epoch 52/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.2092 - accuracy: 0.6066\n",
            "Epoch 53/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.3215 - accuracy: 0.5902\n",
            "Epoch 54/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.2913 - accuracy: 0.5676\n",
            "Epoch 55/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.2046 - accuracy: 0.5881\n",
            "Epoch 56/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.1567 - accuracy: 0.6127\n",
            "Epoch 57/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.1690 - accuracy: 0.6332\n",
            "Epoch 58/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.1962 - accuracy: 0.5881\n",
            "Epoch 59/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.0526 - accuracy: 0.6496\n",
            "Epoch 60/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.1274 - accuracy: 0.6230\n",
            "Epoch 61/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.0087 - accuracy: 0.6680\n",
            "Epoch 62/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.0426 - accuracy: 0.6434\n",
            "Epoch 63/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.1299 - accuracy: 0.6455\n",
            "Epoch 64/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.0484 - accuracy: 0.6250\n",
            "Epoch 65/200\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 1.0397 - accuracy: 0.6209\n",
            "Epoch 66/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.9650 - accuracy: 0.6516\n",
            "Epoch 67/200\n",
            "16/16 [==============================] - 6s 397ms/step - loss: 1.0312 - accuracy: 0.6434\n",
            "Epoch 68/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.1580 - accuracy: 0.6393\n",
            "Epoch 69/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.0906 - accuracy: 0.6475\n",
            "Epoch 70/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.1164 - accuracy: 0.6270\n",
            "Epoch 71/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.9886 - accuracy: 0.6619\n",
            "Epoch 72/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.0324 - accuracy: 0.6680\n",
            "Epoch 73/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.9763 - accuracy: 0.6742\n",
            "Epoch 74/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.0022 - accuracy: 0.6537\n",
            "Epoch 75/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.9363 - accuracy: 0.6988\n",
            "Epoch 76/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.3731 - accuracy: 0.6107\n",
            "Epoch 77/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.1579 - accuracy: 0.6086\n",
            "Epoch 78/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.1942 - accuracy: 0.6148\n",
            "Epoch 79/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.1250 - accuracy: 0.6496\n",
            "Epoch 80/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.0007 - accuracy: 0.6783\n",
            "Epoch 81/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.9290 - accuracy: 0.7008\n",
            "Epoch 82/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.8939 - accuracy: 0.6906\n",
            "Epoch 83/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.8797 - accuracy: 0.6865\n",
            "Epoch 84/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.9093 - accuracy: 0.6967\n",
            "Epoch 85/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.8993 - accuracy: 0.7152\n",
            "Epoch 86/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.9708 - accuracy: 0.6967\n",
            "Epoch 87/200\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.1479 - accuracy: 0.6824\n",
            "Epoch 88/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.9346 - accuracy: 0.6967\n",
            "Epoch 89/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.6828 - accuracy: 0.7787\n",
            "Epoch 90/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.7629 - accuracy: 0.7439\n",
            "Epoch 91/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.8775 - accuracy: 0.7131\n",
            "Epoch 92/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.8834 - accuracy: 0.7254\n",
            "Epoch 93/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.8710 - accuracy: 0.7254\n",
            "Epoch 94/200\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.7749 - accuracy: 0.7541\n",
            "Epoch 95/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.8316 - accuracy: 0.7500\n",
            "Epoch 96/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.7660 - accuracy: 0.7480\n",
            "Epoch 97/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6831 - accuracy: 0.7439\n",
            "Epoch 98/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.7102 - accuracy: 0.7664\n",
            "Epoch 99/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.7095 - accuracy: 0.7664\n",
            "Epoch 100/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.7010 - accuracy: 0.7807\n",
            "Epoch 101/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7311 - accuracy: 0.7828\n",
            "Epoch 102/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.7956 - accuracy: 0.7357\n",
            "Epoch 103/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6559 - accuracy: 0.7992\n",
            "Epoch 104/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6447 - accuracy: 0.7889\n",
            "Epoch 105/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.8162 - accuracy: 0.7602\n",
            "Epoch 106/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7183 - accuracy: 0.7582\n",
            "Epoch 107/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5794 - accuracy: 0.8115\n",
            "Epoch 108/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5237 - accuracy: 0.8320\n",
            "Epoch 109/200\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.6102 - accuracy: 0.7828\n",
            "Epoch 110/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5452 - accuracy: 0.7971\n",
            "Epoch 111/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5318 - accuracy: 0.8381\n",
            "Epoch 112/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5596 - accuracy: 0.8197\n",
            "Epoch 113/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7537 - accuracy: 0.7725\n",
            "Epoch 114/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.7833 - accuracy: 0.7582\n",
            "Epoch 115/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5729 - accuracy: 0.8074\n",
            "Epoch 116/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4896 - accuracy: 0.8381\n",
            "Epoch 117/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4873 - accuracy: 0.8381\n",
            "Epoch 118/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5689 - accuracy: 0.8340\n",
            "Epoch 119/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5248 - accuracy: 0.8217\n",
            "Epoch 120/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6554 - accuracy: 0.7848\n",
            "Epoch 121/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.8805 - accuracy: 0.7254\n",
            "Epoch 122/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6126 - accuracy: 0.7602\n",
            "Epoch 123/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6031 - accuracy: 0.8094\n",
            "Epoch 124/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4760 - accuracy: 0.8381\n",
            "Epoch 125/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5740 - accuracy: 0.8094\n",
            "Epoch 126/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.7103 - accuracy: 0.7561\n",
            "Epoch 127/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6063 - accuracy: 0.8033\n",
            "Epoch 128/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4548 - accuracy: 0.8443\n",
            "Epoch 129/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4467 - accuracy: 0.8689\n",
            "Epoch 130/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4888 - accuracy: 0.8299\n",
            "Epoch 131/200\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.4049 - accuracy: 0.8811\n",
            "Epoch 132/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4792 - accuracy: 0.8627\n",
            "Epoch 133/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4713 - accuracy: 0.8422\n",
            "Epoch 134/200\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5394 - accuracy: 0.8197\n",
            "Epoch 135/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4200 - accuracy: 0.8668\n",
            "Epoch 136/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.4364 - accuracy: 0.8668\n",
            "Epoch 137/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4084 - accuracy: 0.8648\n",
            "Epoch 138/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5100 - accuracy: 0.8176\n",
            "Epoch 139/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.3660 - accuracy: 0.8750\n",
            "Epoch 140/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.4159 - accuracy: 0.8770\n",
            "Epoch 141/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4976 - accuracy: 0.8463\n",
            "Epoch 142/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.3888 - accuracy: 0.8791\n",
            "Epoch 143/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.3715 - accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4431 - accuracy: 0.8566\n",
            "Epoch 145/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.4564 - accuracy: 0.8545\n",
            "Epoch 146/200\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.4325 - accuracy: 0.8648\n",
            "Epoch 147/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6314 - accuracy: 0.8074\n",
            "Epoch 148/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5728 - accuracy: 0.8299\n",
            "Epoch 149/200\n",
            "16/16 [==============================] - 6s 404ms/step - loss: 0.4620 - accuracy: 0.8566\n",
            "Epoch 150/200\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.3954 - accuracy: 0.8730\n",
            "Epoch 151/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4444 - accuracy: 0.8668\n",
            "Epoch 152/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.4218 - accuracy: 0.8668\n",
            "Epoch 153/200\n",
            "16/16 [==============================] - 6s 404ms/step - loss: 0.3651 - accuracy: 0.8770\n",
            "Epoch 154/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3580 - accuracy: 0.8730\n",
            "Epoch 155/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.3125 - accuracy: 0.8893\n",
            "Epoch 156/200\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.3489 - accuracy: 0.8873\n",
            "Epoch 157/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3508 - accuracy: 0.8955\n",
            "Epoch 158/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4075 - accuracy: 0.8689\n",
            "Epoch 159/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.7074 - accuracy: 0.8566\n",
            "Epoch 160/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5686 - accuracy: 0.8320\n",
            "Epoch 161/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.4403 - accuracy: 0.8668\n",
            "Epoch 162/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3690 - accuracy: 0.8791\n",
            "Epoch 163/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3320 - accuracy: 0.8873\n",
            "Epoch 164/200\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.3793 - accuracy: 0.8893\n",
            "Epoch 165/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3127 - accuracy: 0.8955\n",
            "Epoch 166/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2664 - accuracy: 0.9180\n",
            "Epoch 167/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2684 - accuracy: 0.9098\n",
            "Epoch 168/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3130 - accuracy: 0.9098\n",
            "Epoch 169/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.2871 - accuracy: 0.9221\n",
            "Epoch 170/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4125 - accuracy: 0.8627\n",
            "Epoch 171/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3791 - accuracy: 0.8893\n",
            "Epoch 172/200\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5974 - accuracy: 0.8115\n",
            "Epoch 173/200\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5895 - accuracy: 0.8258\n",
            "Epoch 174/200\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5080 - accuracy: 0.8463\n",
            "Epoch 175/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5155 - accuracy: 0.8340\n",
            "Epoch 176/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.4430 - accuracy: 0.8525\n",
            "Epoch 177/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3289 - accuracy: 0.9037\n",
            "Epoch 178/200\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 0.3106 - accuracy: 0.8996\n",
            "Epoch 179/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.3444 - accuracy: 0.8914\n",
            "Epoch 180/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.4738 - accuracy: 0.8402\n",
            "Epoch 181/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4205 - accuracy: 0.8668\n",
            "Epoch 182/200\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.4150 - accuracy: 0.8914\n",
            "Epoch 183/200\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3728 - accuracy: 0.8934\n",
            "Epoch 184/200\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 0.3261 - accuracy: 0.8934\n",
            "Epoch 185/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.2967 - accuracy: 0.9119\n",
            "Epoch 186/200\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.2568 - accuracy: 0.9037\n",
            "Epoch 187/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2769 - accuracy: 0.9078\n",
            "Epoch 188/200\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.3206 - accuracy: 0.9139\n",
            "Epoch 189/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.3736 - accuracy: 0.8914\n",
            "Epoch 190/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3253 - accuracy: 0.8914\n",
            "Epoch 191/200\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.2066 - accuracy: 0.9385\n",
            "Epoch 192/200\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.2325 - accuracy: 0.9303\n",
            "Epoch 193/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.2357 - accuracy: 0.9242\n",
            "Epoch 194/200\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.3064 - accuracy: 0.9160\n",
            "Epoch 195/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2221 - accuracy: 0.9303\n",
            "Epoch 196/200\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3641 - accuracy: 0.8811\n",
            "Epoch 197/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.4234 - accuracy: 0.8627\n",
            "Epoch 198/200\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2745 - accuracy: 0.9180\n",
            "Epoch 199/200\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.2424 - accuracy: 0.9180\n",
            "Epoch 200/200\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2459 - accuracy: 0.9180\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d376b8e90>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMya2gUvVijB",
        "outputId": "f63abf9f-4f55-44f7-f309-ce4491b97b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 96ms/step - loss: 2.8604 - accuracy: 0.6829\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.860377311706543, 0.6829268336296082]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기존 모델로 돌리기(위에서 기존 모델이 정확도가 더 높았기 때문)\n",
        "# 에폭은 다시 150으로 수정 후 이미지 증강을 보충하기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=90,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    zoom_range = 0.3,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model2.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQk4lyyHggov",
        "outputId": "3ad1dc0c-a2e1-40e2-fe85-488e7e7c06ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 7s 371ms/step - loss: 2.6611 - accuracy: 0.1127\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 2.3349 - accuracy: 0.2008\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 2.2879 - accuracy: 0.2152\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 2.1332 - accuracy: 0.2746\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 2.0606 - accuracy: 0.3176\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 2.0311 - accuracy: 0.3115\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 2.0133 - accuracy: 0.3176\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 2.0028 - accuracy: 0.2951\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.9182 - accuracy: 0.3545\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 1.8795 - accuracy: 0.3381\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.9137 - accuracy: 0.3525\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 7s 453ms/step - loss: 1.8512 - accuracy: 0.3443\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.8258 - accuracy: 0.3730\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.8462 - accuracy: 0.3586\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.8408 - accuracy: 0.3648\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.7669 - accuracy: 0.4016\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.7620 - accuracy: 0.3934\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.7503 - accuracy: 0.4119\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.8089 - accuracy: 0.3811\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 1.7694 - accuracy: 0.3934\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.7400 - accuracy: 0.3975\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.6788 - accuracy: 0.4385\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.7098 - accuracy: 0.4098\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 1.7687 - accuracy: 0.4406\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.7027 - accuracy: 0.4221\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.7813 - accuracy: 0.4016\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.7952 - accuracy: 0.3955\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.7994 - accuracy: 0.4139\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.7617 - accuracy: 0.3914\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.6744 - accuracy: 0.4037\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.6774 - accuracy: 0.3996\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.6424 - accuracy: 0.4590\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.6114 - accuracy: 0.4324\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.6239 - accuracy: 0.4529\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.6166 - accuracy: 0.4590\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.5716 - accuracy: 0.4508\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 1.6372 - accuracy: 0.4549\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.5321 - accuracy: 0.4795\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.5043 - accuracy: 0.4652\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.6435 - accuracy: 0.4262\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 1.5867 - accuracy: 0.4406\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.5369 - accuracy: 0.4775\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.5646 - accuracy: 0.4488\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 1.6042 - accuracy: 0.4529\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.6208 - accuracy: 0.4201\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.5658 - accuracy: 0.4447\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.4758 - accuracy: 0.4980\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.5152 - accuracy: 0.4857\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.5671 - accuracy: 0.4611\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.5789 - accuracy: 0.4652\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.4604 - accuracy: 0.5000\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.4742 - accuracy: 0.4713\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.4488 - accuracy: 0.4734\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.4977 - accuracy: 0.4549\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.5130 - accuracy: 0.5000\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.5221 - accuracy: 0.4857\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.4294 - accuracy: 0.4959\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.3768 - accuracy: 0.4939\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.3505 - accuracy: 0.5225\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.3826 - accuracy: 0.5225\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.5096 - accuracy: 0.4877\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.5174 - accuracy: 0.4652\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.4648 - accuracy: 0.4898\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.4011 - accuracy: 0.5164\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 1.3984 - accuracy: 0.5164\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.2941 - accuracy: 0.5512\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3613 - accuracy: 0.5307\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.3709 - accuracy: 0.5512\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.2676 - accuracy: 0.5553\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2680 - accuracy: 0.5717\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.3032 - accuracy: 0.5717\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 6s 406ms/step - loss: 1.5048 - accuracy: 0.4877\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3875 - accuracy: 0.5184\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.2898 - accuracy: 0.5635\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 6s 402ms/step - loss: 1.2681 - accuracy: 0.5820\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.2967 - accuracy: 0.5512\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.3061 - accuracy: 0.5779\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.4321 - accuracy: 0.5102\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.4768 - accuracy: 0.5123\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.3656 - accuracy: 0.5020\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.4299 - accuracy: 0.5574\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3926 - accuracy: 0.5082\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.3155 - accuracy: 0.5594\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2503 - accuracy: 0.5840\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.3350 - accuracy: 0.5512\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.1897 - accuracy: 0.5779\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.1883 - accuracy: 0.6004\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.1430 - accuracy: 0.6066\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.1794 - accuracy: 0.6209\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.2954 - accuracy: 0.6066\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.2520 - accuracy: 0.5615\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.3576 - accuracy: 0.5287\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3450 - accuracy: 0.5656\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.3402 - accuracy: 0.5451\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2772 - accuracy: 0.5389\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2939 - accuracy: 0.5820\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.1786 - accuracy: 0.6127\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.1118 - accuracy: 0.6393\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.0842 - accuracy: 0.6639\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.3180 - accuracy: 0.5635\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.1808 - accuracy: 0.5758\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.0623 - accuracy: 0.6516\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.1195 - accuracy: 0.6352\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.0732 - accuracy: 0.6393\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 1.0957 - accuracy: 0.6537\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.9619 - accuracy: 0.6803\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0366 - accuracy: 0.6619\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9781 - accuracy: 0.6885\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.9197 - accuracy: 0.6947\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 1.1348 - accuracy: 0.6291\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 1.1180 - accuracy: 0.6434\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9967 - accuracy: 0.6803\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9102 - accuracy: 0.7008\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.8370 - accuracy: 0.7213\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.9975 - accuracy: 0.6393\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.8885 - accuracy: 0.7172\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 1.1039 - accuracy: 0.6475\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.9203 - accuracy: 0.7008\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.9361 - accuracy: 0.6701\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.9062 - accuracy: 0.7152\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.8836 - accuracy: 0.7172\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.8486 - accuracy: 0.7193\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0152 - accuracy: 0.6619\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 1.1102 - accuracy: 0.6393\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.9760 - accuracy: 0.7070\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 1.0265 - accuracy: 0.6537\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.9007 - accuracy: 0.7090\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.8523 - accuracy: 0.7070\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 6s 408ms/step - loss: 0.8733 - accuracy: 0.7254\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.8253 - accuracy: 0.7439\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.9084 - accuracy: 0.7090\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.9064 - accuracy: 0.7008\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.7869 - accuracy: 0.7602\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.7771 - accuracy: 0.7520\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.8403 - accuracy: 0.7254\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.7422 - accuracy: 0.7725\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.8422 - accuracy: 0.7439\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.7523 - accuracy: 0.7561\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.7723 - accuracy: 0.7193\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9112 - accuracy: 0.6926\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.8846 - accuracy: 0.7295\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.8741 - accuracy: 0.7254\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.7265 - accuracy: 0.7766\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.8128 - accuracy: 0.7418\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.8758 - accuracy: 0.7152\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.7919 - accuracy: 0.7561\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.7575 - accuracy: 0.7705\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.7205 - accuracy: 0.7705\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6317 - accuracy: 0.8012\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.6638 - accuracy: 0.7828\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d375e0b90>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5G2DiVugh7H",
        "outputId": "55b2033e-a1ee-4dee-ea2d-2c7dff417754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 98ms/step - loss: 2.8971 - accuracy: 0.6179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.897076368331909, 0.6178861856460571]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존모델에서 이미지 증강 낮추기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model2.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEYlJZd-l6yA",
        "outputId": "26f1c76a-f6e9-4c31-f416-48c307a92a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 7s 385ms/step - loss: 2.5819 - accuracy: 0.1209\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 2.3144 - accuracy: 0.1844\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 2.1381 - accuracy: 0.2684\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 1.9783 - accuracy: 0.2992\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.9150 - accuracy: 0.3279\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.8365 - accuracy: 0.3627\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.7892 - accuracy: 0.3750\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 1.6845 - accuracy: 0.4303\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 1.4530 - accuracy: 0.5143\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 1.4355 - accuracy: 0.5061\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3830 - accuracy: 0.5328\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 1.2398 - accuracy: 0.5594\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 1.2620 - accuracy: 0.5635\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 1.2762 - accuracy: 0.5861\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 1.1681 - accuracy: 0.6045\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 1.1276 - accuracy: 0.6291\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 7s 418ms/step - loss: 1.0058 - accuracy: 0.6639\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 1.1087 - accuracy: 0.6373\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.0050 - accuracy: 0.6701\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 1.0200 - accuracy: 0.6598\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.9063 - accuracy: 0.6967\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.9032 - accuracy: 0.6967\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 1.0961 - accuracy: 0.6086\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 1.0304 - accuracy: 0.6619\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.8104 - accuracy: 0.7357\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.9298 - accuracy: 0.6967\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 1.0477 - accuracy: 0.6680\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.9924 - accuracy: 0.6865\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.8977 - accuracy: 0.7111\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.8493 - accuracy: 0.7480\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.8304 - accuracy: 0.7254\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.7459 - accuracy: 0.7459\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.7916 - accuracy: 0.7418\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.8429 - accuracy: 0.7172\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.7197 - accuracy: 0.7643\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.6765 - accuracy: 0.7930\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5924 - accuracy: 0.8299\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5910 - accuracy: 0.8115\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6076 - accuracy: 0.8115\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5528 - accuracy: 0.8258\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5521 - accuracy: 0.8197\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6010 - accuracy: 0.8012\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5795 - accuracy: 0.8094\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5573 - accuracy: 0.8156\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.4434 - accuracy: 0.8709\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.4463 - accuracy: 0.8443\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.6179 - accuracy: 0.8279\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5426 - accuracy: 0.8443\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5294 - accuracy: 0.8320\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4359 - accuracy: 0.8648\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3916 - accuracy: 0.8730\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.4042 - accuracy: 0.8545\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.4939 - accuracy: 0.8361\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4234 - accuracy: 0.8770\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.4040 - accuracy: 0.8873\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.3962 - accuracy: 0.8750\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3535 - accuracy: 0.8770\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3117 - accuracy: 0.9016\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.3051 - accuracy: 0.9016\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.4359 - accuracy: 0.8873\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.4634 - accuracy: 0.8545\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.4123 - accuracy: 0.8730\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.2576 - accuracy: 0.9139\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4394 - accuracy: 0.8381\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.3850 - accuracy: 0.8545\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.3613 - accuracy: 0.8791\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4495 - accuracy: 0.8811\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.2479 - accuracy: 0.9324\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.2133 - accuracy: 0.9283\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.2898 - accuracy: 0.9119\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.3055 - accuracy: 0.8975\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.3225 - accuracy: 0.8934\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.3955 - accuracy: 0.8750\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3450 - accuracy: 0.8934\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.2781 - accuracy: 0.9057\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.2387 - accuracy: 0.9160\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.2332 - accuracy: 0.9385\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 6s 396ms/step - loss: 0.1373 - accuracy: 0.9488\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.2094 - accuracy: 0.9508\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.2131 - accuracy: 0.9365\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.2753 - accuracy: 0.9262\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.3237 - accuracy: 0.9119\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.3087 - accuracy: 0.8975\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.3322 - accuracy: 0.9160\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.3897 - accuracy: 0.9324\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.2872 - accuracy: 0.9324\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1901 - accuracy: 0.9488\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.2377 - accuracy: 0.9385\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1655 - accuracy: 0.9447\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.2011 - accuracy: 0.9488\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.2576 - accuracy: 0.9385\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.2587 - accuracy: 0.9201\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.2910 - accuracy: 0.9221\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.3115 - accuracy: 0.9221\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.2331 - accuracy: 0.9365\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.2000 - accuracy: 0.9303\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.1671 - accuracy: 0.9529\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.1928 - accuracy: 0.9385\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.1532 - accuracy: 0.9590\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1651 - accuracy: 0.9529\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1263 - accuracy: 0.9529\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.1460 - accuracy: 0.9508\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.1310 - accuracy: 0.9590\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.1051 - accuracy: 0.9631\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1338 - accuracy: 0.9590\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1432 - accuracy: 0.9713\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1632 - accuracy: 0.9529\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.2221 - accuracy: 0.9324\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.1234 - accuracy: 0.9631\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.1602 - accuracy: 0.9570\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.1189 - accuracy: 0.9570\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1809 - accuracy: 0.9365\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.1347 - accuracy: 0.9590\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1115 - accuracy: 0.9631\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.1096 - accuracy: 0.9611\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.1387 - accuracy: 0.9570\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.0854 - accuracy: 0.9754\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1212 - accuracy: 0.9713\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.0771 - accuracy: 0.9795\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1061 - accuracy: 0.9672\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.1148 - accuracy: 0.9570\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.2026 - accuracy: 0.9447\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1690 - accuracy: 0.9488\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.1532 - accuracy: 0.9467\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1586 - accuracy: 0.9529\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1692 - accuracy: 0.9672\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.1247 - accuracy: 0.9672\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.0806 - accuracy: 0.9775\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.1711 - accuracy: 0.9508\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1850 - accuracy: 0.9426\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.1102 - accuracy: 0.9590\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.2529 - accuracy: 0.9303\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.2131 - accuracy: 0.9385\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.6152 - accuracy: 0.8832\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.4196 - accuracy: 0.8852\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.2677 - accuracy: 0.9098\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.1891 - accuracy: 0.9344\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.3759 - accuracy: 0.8955\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.2309 - accuracy: 0.9160\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.2151 - accuracy: 0.9529\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.1886 - accuracy: 0.9467\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 6s 396ms/step - loss: 0.1613 - accuracy: 0.9631\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.0994 - accuracy: 0.9713\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.0831 - accuracy: 0.9775\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.1045 - accuracy: 0.9734\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.2807 - accuracy: 0.9344\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.2017 - accuracy: 0.9426\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.1127 - accuracy: 0.9713\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.1237 - accuracy: 0.9652\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.0853 - accuracy: 0.9713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d3633ab10>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE7psU0zmEcW",
        "outputId": "679b34d5-0026-4fda-98e3-bf9ecff8a33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 93ms/step - loss: 4.2216 - accuracy: 0.6585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.221582412719727, 0.6585366129875183]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#두번째 모델\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "model3 = models.Sequential()\n",
        "\n",
        "model3.add(layers.Conv2D(32, (3, 3), input_shape=(224,224,3)))\n",
        "model3.add(layers.Activation('relu'))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model3.add(layers.Conv2D(32, (3, 3)))\n",
        "model3.add(layers.Activation('relu'))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model3.add(layers.Conv2D(64, (3, 3)))\n",
        "model3.add(layers.Activation('relu'))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model3.add(layers.Flatten()) # Output convert into one dimension layer and will go to Dense layer\n",
        "model3.add(layers.Dense(64))\n",
        "model3.add(layers.Activation('relu'))\n",
        "model3.add(layers.Dropout(0.5))\n",
        "model3.add(layers.Dense(len(folder_name)))\n",
        "model3.add(layers.Activation('softmax'))\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', \n",
        "            optimizer=\"adam\",\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfKfv04LqCMS",
        "outputId": "3f2db319-378c-4165-bfcb-ef743185784a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 222, 222, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 111, 111, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 109, 109, 32)      9248      \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 109, 109, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 54, 54, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 52, 52, 64)        18496     \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 52, 52, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 26, 26, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 43264)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                2768960   \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 12)                780       \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 12)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,798,380\n",
            "Trainable params: 2,798,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#두번째 모델을 이미지 증강 적당히해서 돌리기(가장 높게 나온걸로)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model3.fit(train_gen, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNcPNH3qqIH3",
        "outputId": "428255aa-2721-406c-bb60-8aa112656091"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 17s 286ms/step - loss: 2.5653 - accuracy: 0.1127\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 2.4622 - accuracy: 0.0943\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.3804 - accuracy: 0.1250\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.3564 - accuracy: 0.1352\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 2.3093 - accuracy: 0.1742\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 2.2425 - accuracy: 0.2275\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 2.2190 - accuracy: 0.2152\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 2.1507 - accuracy: 0.2541\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.1336 - accuracy: 0.2316\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.1086 - accuracy: 0.2705\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.1563 - accuracy: 0.2500\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.1299 - accuracy: 0.2295\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.0603 - accuracy: 0.2602\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 2.0464 - accuracy: 0.2807\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.0613 - accuracy: 0.2725\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.0183 - accuracy: 0.2766\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 2.0814 - accuracy: 0.2787\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.0134 - accuracy: 0.2746\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.9885 - accuracy: 0.2848\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9921 - accuracy: 0.2930\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.0457 - accuracy: 0.2807\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 2.0094 - accuracy: 0.2828\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.9309 - accuracy: 0.3053\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.9394 - accuracy: 0.3074\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9557 - accuracy: 0.3012\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.9291 - accuracy: 0.3504\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8407 - accuracy: 0.3525\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.8358 - accuracy: 0.3545\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8117 - accuracy: 0.3566\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8171 - accuracy: 0.3750\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8085 - accuracy: 0.3852\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.8484 - accuracy: 0.3627\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 1.8477 - accuracy: 0.3463\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.8303 - accuracy: 0.3689\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.7704 - accuracy: 0.3402\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.7594 - accuracy: 0.4078\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.7365 - accuracy: 0.4037\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.7388 - accuracy: 0.3996\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.7537 - accuracy: 0.3893\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 1.7038 - accuracy: 0.3914\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.7155 - accuracy: 0.4037\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.7141 - accuracy: 0.3873\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.6627 - accuracy: 0.4221\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.6808 - accuracy: 0.4057\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.6627 - accuracy: 0.4303\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.6680 - accuracy: 0.4160\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.6801 - accuracy: 0.3975\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.6490 - accuracy: 0.4303\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.6392 - accuracy: 0.4426\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.5512 - accuracy: 0.4365\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.5728 - accuracy: 0.4857\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.6025 - accuracy: 0.4221\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.5027 - accuracy: 0.4836\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.5609 - accuracy: 0.4529\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.5786 - accuracy: 0.4508\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.5544 - accuracy: 0.4631\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.6494 - accuracy: 0.4221\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.6012 - accuracy: 0.4324\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.5922 - accuracy: 0.4508\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.4637 - accuracy: 0.4795\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.6101 - accuracy: 0.4385\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.5400 - accuracy: 0.4549\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.6080 - accuracy: 0.4488\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.4831 - accuracy: 0.4959\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.4742 - accuracy: 0.4918\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.5906 - accuracy: 0.4324\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.4581 - accuracy: 0.5266\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.4758 - accuracy: 0.5184\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.4634 - accuracy: 0.4918\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.4016 - accuracy: 0.5020\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.4434 - accuracy: 0.5082\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.4089 - accuracy: 0.5041\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.4162 - accuracy: 0.5020\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.4442 - accuracy: 0.5123\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.4611 - accuracy: 0.4959\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.3235 - accuracy: 0.5512\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.4265 - accuracy: 0.5123\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.3975 - accuracy: 0.5266\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.4238 - accuracy: 0.5041\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.3063 - accuracy: 0.5553\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.3781 - accuracy: 0.5348\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3477 - accuracy: 0.5369\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.2904 - accuracy: 0.5533\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.4432 - accuracy: 0.4857\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.4222 - accuracy: 0.4959\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.3790 - accuracy: 0.5123\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3012 - accuracy: 0.5389\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3613 - accuracy: 0.5287\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3272 - accuracy: 0.5266\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.3188 - accuracy: 0.5328\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.3845 - accuracy: 0.5369\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.3293 - accuracy: 0.5307\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.2462 - accuracy: 0.5594\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.3305 - accuracy: 0.5533\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.2427 - accuracy: 0.5840\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.3078 - accuracy: 0.5225\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.2977 - accuracy: 0.5615\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.4455 - accuracy: 0.4959\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.4451 - accuracy: 0.4980\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.3619 - accuracy: 0.5594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16ac3ab110>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhA8lVO8vcdg",
        "outputId": "ceecbb56-d203-4085-d207-e061b305e8cc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 110ms/step - loss: 1.5327 - accuracy: 0.6098\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5326859951019287, 0.6097561120986938]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#두번째 모델을 이미지 증강 적당히해서 돌리기(가장 높게 나온걸로)\n",
        "#epoch를 150으로 늘리기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model3.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7DaRLQlvkTM",
        "outputId": "1b8e38ff-845a-44c6-c904-85e7f0f8a5cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.5782 - accuracy: 0.0840\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.3914 - accuracy: 0.1578\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 2.2792 - accuracy: 0.1906\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 2.2437 - accuracy: 0.2131\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 2.1942 - accuracy: 0.2520\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.1338 - accuracy: 0.2500\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.1162 - accuracy: 0.2602\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 2.0498 - accuracy: 0.2664\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 2.0358 - accuracy: 0.2992\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.0080 - accuracy: 0.3033\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.9647 - accuracy: 0.3094\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.9691 - accuracy: 0.3299\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.9561 - accuracy: 0.3238\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8921 - accuracy: 0.3484\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.9477 - accuracy: 0.3156\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8559 - accuracy: 0.3504\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.8597 - accuracy: 0.3504\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8579 - accuracy: 0.3402\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 1.8911 - accuracy: 0.3176\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.8550 - accuracy: 0.3545\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.8311 - accuracy: 0.3607\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8286 - accuracy: 0.3607\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8048 - accuracy: 0.3525\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.7861 - accuracy: 0.3525\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.7119 - accuracy: 0.4303\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.7226 - accuracy: 0.4201\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.6548 - accuracy: 0.4180\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.6784 - accuracy: 0.4406\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.6530 - accuracy: 0.4221\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.6584 - accuracy: 0.4037\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.6804 - accuracy: 0.4426\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.6311 - accuracy: 0.4365\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.5752 - accuracy: 0.4590\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.5903 - accuracy: 0.4611\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.4990 - accuracy: 0.4918\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.4840 - accuracy: 0.4652\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.5526 - accuracy: 0.4426\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.5823 - accuracy: 0.4385\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.5934 - accuracy: 0.4467\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.4713 - accuracy: 0.4877\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.5247 - accuracy: 0.4918\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.4794 - accuracy: 0.4775\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.4960 - accuracy: 0.4508\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.5299 - accuracy: 0.5102\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.4188 - accuracy: 0.4939\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.4524 - accuracy: 0.5020\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.4531 - accuracy: 0.4836\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.4975 - accuracy: 0.5000\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.4274 - accuracy: 0.5246\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.3326 - accuracy: 0.5430\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.4614 - accuracy: 0.5041\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3642 - accuracy: 0.5348\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.3796 - accuracy: 0.5225\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 5s 323ms/step - loss: 1.4263 - accuracy: 0.4980\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.4384 - accuracy: 0.5061\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.4477 - accuracy: 0.5020\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.3897 - accuracy: 0.5123\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3181 - accuracy: 0.5553\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.4409 - accuracy: 0.5020\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.4316 - accuracy: 0.5082\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.3832 - accuracy: 0.5123\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.4001 - accuracy: 0.5389\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3446 - accuracy: 0.5369\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.2825 - accuracy: 0.5656\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.2848 - accuracy: 0.5369\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.2587 - accuracy: 0.5738\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.2249 - accuracy: 0.5635\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.2409 - accuracy: 0.5635\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.2564 - accuracy: 0.5553\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.3541 - accuracy: 0.5246\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.3726 - accuracy: 0.5369\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.3101 - accuracy: 0.5492\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.2577 - accuracy: 0.5328\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.2010 - accuracy: 0.5799\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.3435 - accuracy: 0.5635\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.2756 - accuracy: 0.5512\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.1952 - accuracy: 0.5553\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.1541 - accuracy: 0.5840\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.1774 - accuracy: 0.6148\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.2368 - accuracy: 0.5840\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.1486 - accuracy: 0.6086\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.1228 - accuracy: 0.6004\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.1896 - accuracy: 0.6066\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.3908 - accuracy: 0.5266\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.2013 - accuracy: 0.6045\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.1526 - accuracy: 0.5779\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.1636 - accuracy: 0.5594\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.1172 - accuracy: 0.6127\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.1271 - accuracy: 0.6045\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.1548 - accuracy: 0.5943\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.0691 - accuracy: 0.6393\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.1006 - accuracy: 0.5984\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 1.1978 - accuracy: 0.5943\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.2132 - accuracy: 0.5820\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.1954 - accuracy: 0.5881\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.1574 - accuracy: 0.5738\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.1696 - accuracy: 0.5881\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.1843 - accuracy: 0.5861\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.2796 - accuracy: 0.5410\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.1074 - accuracy: 0.5963\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.1675 - accuracy: 0.5943\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.1101 - accuracy: 0.5943\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.0981 - accuracy: 0.6086\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 1.1136 - accuracy: 0.6066\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.0601 - accuracy: 0.6291\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.0468 - accuracy: 0.6496\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.0999 - accuracy: 0.6455\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 1.0326 - accuracy: 0.6107\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.9423 - accuracy: 0.6680\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.0958 - accuracy: 0.6148\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.0835 - accuracy: 0.6127\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.0164 - accuracy: 0.6393\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 0.9386 - accuracy: 0.6496\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.0298 - accuracy: 0.6230\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.1108 - accuracy: 0.6168\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.0946 - accuracy: 0.6189\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.0697 - accuracy: 0.6250\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.9911 - accuracy: 0.6475\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.0219 - accuracy: 0.6414\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.9906 - accuracy: 0.6537\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.0227 - accuracy: 0.6393\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.9839 - accuracy: 0.6680\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.0022 - accuracy: 0.6537\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.9770 - accuracy: 0.6578\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.9678 - accuracy: 0.6824\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.9595 - accuracy: 0.6557\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.0031 - accuracy: 0.6619\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 0.8897 - accuracy: 0.6947\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.9241 - accuracy: 0.6434\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.9207 - accuracy: 0.6762\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.8993 - accuracy: 0.6803\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.8455 - accuracy: 0.6803\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 0.9719 - accuracy: 0.6619\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.9350 - accuracy: 0.6721\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.0072 - accuracy: 0.6639\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.0525 - accuracy: 0.6250\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.9297 - accuracy: 0.6455\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 0.9353 - accuracy: 0.6680\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.8808 - accuracy: 0.6639\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.9662 - accuracy: 0.6578\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.1132 - accuracy: 0.6230\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.9671 - accuracy: 0.6762\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 0.9514 - accuracy: 0.6742\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 0.9181 - accuracy: 0.7029\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 0.8718 - accuracy: 0.6967\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.8643 - accuracy: 0.6844\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.9576 - accuracy: 0.6578\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.9142 - accuracy: 0.6721\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.9762 - accuracy: 0.6639\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.0157 - accuracy: 0.6701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16971ff5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UJn34BmvqDb",
        "outputId": "77da8d46-2464-4707-ecff-5d6f6afa56f0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 23ms/step - loss: 1.2532 - accuracy: 0.6179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2532234191894531, 0.6178861856460571]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#세번째 모델\n",
        "#정확도 별로\n",
        "\n",
        "import os, glob, numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Conv2D(32, (3,3), padding=\"same\", input_shape=(224,224,3), activation='relu'))\n",
        "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model4.add(Dropout(0.25))\n",
        "    \n",
        "model4.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
        "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model4.add(Dropout(0.25))\n",
        "    \n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(256, activation='relu'))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(len(folder_name), activation='softmax'))\n",
        "\n",
        "model4.summary()\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq_tV_fM1AK3",
        "outputId": "99cba00b-c2ee-4ad0-c7b3-2d1895fcbef7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_11 (Conv2D)          (None, 224, 224, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 112, 112, 32)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 112, 112, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 56, 56, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 56, 56, 64)        0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 200704)            0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               51380480  \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                3084      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,402,956\n",
            "Trainable params: 51,402,956\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#세번째 모델을 이미지 증강 적당히해서 돌리기(가장 높게 나온걸로)\n",
        "#epoch를 150\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model4.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWbLtaiS1LEh",
        "outputId": "ddde13a7-2de0-4425-ae7d-276598f75053"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 6s 309ms/step - loss: 17.2140 - accuracy: 0.0840\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 2.4470 - accuracy: 0.1127\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 5s 330ms/step - loss: 2.3355 - accuracy: 0.1680\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 2.1965 - accuracy: 0.2152\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 2.1427 - accuracy: 0.2643\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 2.0467 - accuracy: 0.2807\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.9309 - accuracy: 0.3443\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.8948 - accuracy: 0.3648\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.8696 - accuracy: 0.3586\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.7514 - accuracy: 0.3770\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.8387 - accuracy: 0.4057\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.6970 - accuracy: 0.4078\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.6928 - accuracy: 0.3873\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.7189 - accuracy: 0.4242\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 6s 422ms/step - loss: 1.6891 - accuracy: 0.3914\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.6466 - accuracy: 0.4344\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.6062 - accuracy: 0.4529\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.5939 - accuracy: 0.4057\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.5970 - accuracy: 0.3955\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.5882 - accuracy: 0.4365\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.5749 - accuracy: 0.4447\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.5994 - accuracy: 0.4139\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.5048 - accuracy: 0.4652\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.5277 - accuracy: 0.4529\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.6112 - accuracy: 0.4365\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.5584 - accuracy: 0.4590\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.6109 - accuracy: 0.4590\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.5462 - accuracy: 0.4529\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.4759 - accuracy: 0.4549\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.5467 - accuracy: 0.4488\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.4432 - accuracy: 0.4754\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.4513 - accuracy: 0.5000\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.4018 - accuracy: 0.4980\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.5302 - accuracy: 0.4713\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.4172 - accuracy: 0.4898\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.4134 - accuracy: 0.5041\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3814 - accuracy: 0.5184\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.3918 - accuracy: 0.5102\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3809 - accuracy: 0.5287\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.4037 - accuracy: 0.5082\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3764 - accuracy: 0.5082\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.4078 - accuracy: 0.5082\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.3759 - accuracy: 0.4939\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2728 - accuracy: 0.5738\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3616 - accuracy: 0.5184\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3300 - accuracy: 0.5369\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.2788 - accuracy: 0.5574\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.3850 - accuracy: 0.5020\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2860 - accuracy: 0.5307\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.3731 - accuracy: 0.5246\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.3314 - accuracy: 0.5389\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.3738 - accuracy: 0.5184\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.3341 - accuracy: 0.5389\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.2778 - accuracy: 0.5697\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 1.2465 - accuracy: 0.5738\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2380 - accuracy: 0.5697\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 1.3504 - accuracy: 0.5594\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2967 - accuracy: 0.5594\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.3223 - accuracy: 0.5307\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.2127 - accuracy: 0.5635\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.2225 - accuracy: 0.5820\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.2154 - accuracy: 0.5902\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.1711 - accuracy: 0.6311\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.1854 - accuracy: 0.6004\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2193 - accuracy: 0.5861\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.2445 - accuracy: 0.5881\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.2121 - accuracy: 0.5943\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.2190 - accuracy: 0.5881\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 1.3005 - accuracy: 0.5717\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.1684 - accuracy: 0.6107\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 1.1636 - accuracy: 0.6127\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0777 - accuracy: 0.6270\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 1.1995 - accuracy: 0.5963\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.0653 - accuracy: 0.6270\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.1455 - accuracy: 0.6066\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.1143 - accuracy: 0.6291\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.1011 - accuracy: 0.6168\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.2045 - accuracy: 0.6045\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.0737 - accuracy: 0.6045\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.1006 - accuracy: 0.6291\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.0554 - accuracy: 0.6414\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0180 - accuracy: 0.6373\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0650 - accuracy: 0.6393\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.0449 - accuracy: 0.6230\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.1430 - accuracy: 0.6045\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.1316 - accuracy: 0.6209\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.0762 - accuracy: 0.6025\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0662 - accuracy: 0.6393\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.0815 - accuracy: 0.6414\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0697 - accuracy: 0.6045\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 1.1619 - accuracy: 0.5943\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0182 - accuracy: 0.6393\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.9776 - accuracy: 0.6619\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 1.0029 - accuracy: 0.6803\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.0906 - accuracy: 0.6291\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 5s 327ms/step - loss: 1.0495 - accuracy: 0.6434\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0410 - accuracy: 0.6414\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 1.0771 - accuracy: 0.6189\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.0090 - accuracy: 0.6762\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0362 - accuracy: 0.6434\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0227 - accuracy: 0.6598\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.1143 - accuracy: 0.6434\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0642 - accuracy: 0.6332\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.0165 - accuracy: 0.6496\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.9782 - accuracy: 0.6578\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.9486 - accuracy: 0.6639\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.9565 - accuracy: 0.6537\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.9709 - accuracy: 0.6639\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.8956 - accuracy: 0.6926\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.0125 - accuracy: 0.6537\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.0306 - accuracy: 0.6332\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.0207 - accuracy: 0.6434\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.9613 - accuracy: 0.6824\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.9555 - accuracy: 0.6742\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.9074 - accuracy: 0.6844\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.0036 - accuracy: 0.6434\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.9064 - accuracy: 0.6721\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 0.9458 - accuracy: 0.6475\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.9152 - accuracy: 0.6824\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 0.9468 - accuracy: 0.6967\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.9738 - accuracy: 0.6701\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.9127 - accuracy: 0.6598\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.8893 - accuracy: 0.6742\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.8475 - accuracy: 0.6947\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8904 - accuracy: 0.6885\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.8885 - accuracy: 0.6762\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.9494 - accuracy: 0.6988\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8683 - accuracy: 0.6865\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8228 - accuracy: 0.7213\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.9864 - accuracy: 0.6619\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.8890 - accuracy: 0.7131\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 0.8662 - accuracy: 0.7070\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.8826 - accuracy: 0.6844\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8406 - accuracy: 0.6906\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8781 - accuracy: 0.7172\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.9693 - accuracy: 0.6680\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.8955 - accuracy: 0.7008\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8938 - accuracy: 0.6885\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.9287 - accuracy: 0.6701\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.8337 - accuracy: 0.7193\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.8259 - accuracy: 0.7070\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.9096 - accuracy: 0.7070\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.9402 - accuracy: 0.6947\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.8801 - accuracy: 0.6885\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.9379 - accuracy: 0.6824\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.8500 - accuracy: 0.7029\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.8630 - accuracy: 0.7090\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8378 - accuracy: 0.6967\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.7746 - accuracy: 0.7582\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8242 - accuracy: 0.7131\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f169706ad50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz3rrGwO8iHu",
        "outputId": "8740f6a9-5848-4e50-dfd3-68343d2c2e98"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 117ms/step - loss: 2.0310 - accuracy: 0.6585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0309622287750244, 0.6585366129875183]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#세번째 모델을 이미지 증강 적당히해서 돌리기(가장 높게 나온걸로)\n",
        "#epoch를 250\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model4.fit(train_gen, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIl9GOgW8pzv",
        "outputId": "b4fac1b2-92a8-4895-979f-a5c41df8b036"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "16/16 [==============================] - 6s 316ms/step - loss: 9.7255 - accuracy: 0.0963\n",
            "Epoch 2/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 2.4604 - accuracy: 0.1189\n",
            "Epoch 3/250\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 2.3276 - accuracy: 0.1639\n",
            "Epoch 4/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 2.2100 - accuracy: 0.1967\n",
            "Epoch 5/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 2.1730 - accuracy: 0.2500\n",
            "Epoch 6/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 2.0835 - accuracy: 0.2807\n",
            "Epoch 7/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 2.0295 - accuracy: 0.2992\n",
            "Epoch 8/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 2.0412 - accuracy: 0.3156\n",
            "Epoch 9/250\n",
            "16/16 [==============================] - 5s 329ms/step - loss: 1.8630 - accuracy: 0.3484\n",
            "Epoch 10/250\n",
            "16/16 [==============================] - 5s 328ms/step - loss: 1.8870 - accuracy: 0.3320\n",
            "Epoch 11/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.9072 - accuracy: 0.3586\n",
            "Epoch 12/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.8792 - accuracy: 0.3402\n",
            "Epoch 13/250\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 1.7929 - accuracy: 0.3791\n",
            "Epoch 14/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.7926 - accuracy: 0.3525\n",
            "Epoch 15/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.7675 - accuracy: 0.3893\n",
            "Epoch 16/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.6634 - accuracy: 0.4119\n",
            "Epoch 17/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.6560 - accuracy: 0.4119\n",
            "Epoch 18/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.6664 - accuracy: 0.4078\n",
            "Epoch 19/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.6384 - accuracy: 0.4488\n",
            "Epoch 20/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.6321 - accuracy: 0.4549\n",
            "Epoch 21/250\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.5896 - accuracy: 0.4426\n",
            "Epoch 22/250\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.5658 - accuracy: 0.4713\n",
            "Epoch 23/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.6405 - accuracy: 0.4283\n",
            "Epoch 24/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 1.5199 - accuracy: 0.4816\n",
            "Epoch 25/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.5883 - accuracy: 0.4488\n",
            "Epoch 26/250\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.4894 - accuracy: 0.4959\n",
            "Epoch 27/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.5238 - accuracy: 0.4918\n",
            "Epoch 28/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.5302 - accuracy: 0.4652\n",
            "Epoch 29/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.4923 - accuracy: 0.4816\n",
            "Epoch 30/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.4748 - accuracy: 0.5184\n",
            "Epoch 31/250\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.4929 - accuracy: 0.4775\n",
            "Epoch 32/250\n",
            "16/16 [==============================] - 5s 332ms/step - loss: 1.4636 - accuracy: 0.5041\n",
            "Epoch 33/250\n",
            "16/16 [==============================] - 5s 326ms/step - loss: 1.3938 - accuracy: 0.5266\n",
            "Epoch 34/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.4743 - accuracy: 0.5143\n",
            "Epoch 35/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.3853 - accuracy: 0.5246\n",
            "Epoch 36/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.4053 - accuracy: 0.5102\n",
            "Epoch 37/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.3520 - accuracy: 0.5143\n",
            "Epoch 38/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.3691 - accuracy: 0.5635\n",
            "Epoch 39/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.3626 - accuracy: 0.5348\n",
            "Epoch 40/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.3750 - accuracy: 0.5369\n",
            "Epoch 41/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.3284 - accuracy: 0.5676\n",
            "Epoch 42/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.3015 - accuracy: 0.5697\n",
            "Epoch 43/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.2567 - accuracy: 0.5840\n",
            "Epoch 44/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 1.4220 - accuracy: 0.5184\n",
            "Epoch 45/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.3322 - accuracy: 0.5348\n",
            "Epoch 46/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2939 - accuracy: 0.5574\n",
            "Epoch 47/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.2373 - accuracy: 0.5840\n",
            "Epoch 48/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.2450 - accuracy: 0.5758\n",
            "Epoch 49/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.2075 - accuracy: 0.6004\n",
            "Epoch 50/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.2447 - accuracy: 0.5574\n",
            "Epoch 51/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.2033 - accuracy: 0.5738\n",
            "Epoch 52/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.2105 - accuracy: 0.5840\n",
            "Epoch 53/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.2281 - accuracy: 0.5779\n",
            "Epoch 54/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2133 - accuracy: 0.5799\n",
            "Epoch 55/250\n",
            "16/16 [==============================] - 5s 332ms/step - loss: 1.2185 - accuracy: 0.5799\n",
            "Epoch 56/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.2589 - accuracy: 0.5963\n",
            "Epoch 57/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.2968 - accuracy: 0.5840\n",
            "Epoch 58/250\n",
            "16/16 [==============================] - 5s 328ms/step - loss: 1.1581 - accuracy: 0.6352\n",
            "Epoch 59/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.2036 - accuracy: 0.5922\n",
            "Epoch 60/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.1438 - accuracy: 0.6148\n",
            "Epoch 61/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.2480 - accuracy: 0.5840\n",
            "Epoch 62/250\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.2902 - accuracy: 0.5840\n",
            "Epoch 63/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.2370 - accuracy: 0.5717\n",
            "Epoch 64/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.1779 - accuracy: 0.5861\n",
            "Epoch 65/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 1.1581 - accuracy: 0.6107\n",
            "Epoch 66/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 1.2034 - accuracy: 0.5820\n",
            "Epoch 67/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 1.1452 - accuracy: 0.6107\n",
            "Epoch 68/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.0641 - accuracy: 0.6352\n",
            "Epoch 69/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.1608 - accuracy: 0.6189\n",
            "Epoch 70/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.0877 - accuracy: 0.6189\n",
            "Epoch 71/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.1309 - accuracy: 0.6107\n",
            "Epoch 72/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.0872 - accuracy: 0.6270\n",
            "Epoch 73/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.1249 - accuracy: 0.6209\n",
            "Epoch 74/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.0649 - accuracy: 0.6168\n",
            "Epoch 75/250\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 1.0665 - accuracy: 0.6434\n",
            "Epoch 76/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0773 - accuracy: 0.6332\n",
            "Epoch 77/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0174 - accuracy: 0.6496\n",
            "Epoch 78/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.0540 - accuracy: 0.6455\n",
            "Epoch 79/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 1.0532 - accuracy: 0.6414\n",
            "Epoch 80/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.0459 - accuracy: 0.6475\n",
            "Epoch 81/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 1.2530 - accuracy: 0.6004\n",
            "Epoch 82/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 1.0772 - accuracy: 0.6434\n",
            "Epoch 83/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.9980 - accuracy: 0.6598\n",
            "Epoch 84/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.0429 - accuracy: 0.6332\n",
            "Epoch 85/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 1.0355 - accuracy: 0.6352\n",
            "Epoch 86/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.9857 - accuracy: 0.6660\n",
            "Epoch 87/250\n",
            "16/16 [==============================] - 5s 326ms/step - loss: 0.9751 - accuracy: 0.6680\n",
            "Epoch 88/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 1.1043 - accuracy: 0.6537\n",
            "Epoch 89/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.0971 - accuracy: 0.6352\n",
            "Epoch 90/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.9034 - accuracy: 0.6906\n",
            "Epoch 91/250\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 1.0397 - accuracy: 0.6393\n",
            "Epoch 92/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.9475 - accuracy: 0.6680\n",
            "Epoch 93/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.9919 - accuracy: 0.6475\n",
            "Epoch 94/250\n",
            "16/16 [==============================] - 5s 328ms/step - loss: 0.9682 - accuracy: 0.6496\n",
            "Epoch 95/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0036 - accuracy: 0.6783\n",
            "Epoch 96/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 1.0140 - accuracy: 0.6352\n",
            "Epoch 97/250\n",
            "16/16 [==============================] - 5s 323ms/step - loss: 0.8945 - accuracy: 0.7029\n",
            "Epoch 98/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.9481 - accuracy: 0.6598\n",
            "Epoch 99/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.8947 - accuracy: 0.6967\n",
            "Epoch 100/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.9640 - accuracy: 0.6721\n",
            "Epoch 101/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8997 - accuracy: 0.7152\n",
            "Epoch 102/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.9117 - accuracy: 0.7070\n",
            "Epoch 103/250\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 0.9402 - accuracy: 0.6824\n",
            "Epoch 104/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 1.0845 - accuracy: 0.6291\n",
            "Epoch 105/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.9630 - accuracy: 0.6660\n",
            "Epoch 106/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 1.0204 - accuracy: 0.6496\n",
            "Epoch 107/250\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 1.0168 - accuracy: 0.6455\n",
            "Epoch 108/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.9347 - accuracy: 0.6680\n",
            "Epoch 109/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 1.0173 - accuracy: 0.6619\n",
            "Epoch 110/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.9500 - accuracy: 0.6721\n",
            "Epoch 111/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.9567 - accuracy: 0.6947\n",
            "Epoch 112/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.9548 - accuracy: 0.6701\n",
            "Epoch 113/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8287 - accuracy: 0.7131\n",
            "Epoch 114/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.9166 - accuracy: 0.7131\n",
            "Epoch 115/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.9282 - accuracy: 0.6824\n",
            "Epoch 116/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.8534 - accuracy: 0.7029\n",
            "Epoch 117/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8769 - accuracy: 0.7008\n",
            "Epoch 118/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.8434 - accuracy: 0.7029\n",
            "Epoch 119/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.7988 - accuracy: 0.7316\n",
            "Epoch 120/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.8133 - accuracy: 0.7193\n",
            "Epoch 121/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 0.8585 - accuracy: 0.6783\n",
            "Epoch 122/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.8378 - accuracy: 0.7029\n",
            "Epoch 123/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8456 - accuracy: 0.7152\n",
            "Epoch 124/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8997 - accuracy: 0.6783\n",
            "Epoch 125/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8516 - accuracy: 0.6947\n",
            "Epoch 126/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.9325 - accuracy: 0.6988\n",
            "Epoch 127/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8510 - accuracy: 0.6967\n",
            "Epoch 128/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.8500 - accuracy: 0.7090\n",
            "Epoch 129/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.9045 - accuracy: 0.6947\n",
            "Epoch 130/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8107 - accuracy: 0.7234\n",
            "Epoch 131/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.8505 - accuracy: 0.7152\n",
            "Epoch 132/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8348 - accuracy: 0.6947\n",
            "Epoch 133/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8430 - accuracy: 0.6906\n",
            "Epoch 134/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.8519 - accuracy: 0.6947\n",
            "Epoch 135/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8801 - accuracy: 0.7234\n",
            "Epoch 136/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.7818 - accuracy: 0.7111\n",
            "Epoch 137/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8293 - accuracy: 0.7049\n",
            "Epoch 138/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.7934 - accuracy: 0.7398\n",
            "Epoch 139/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.8088 - accuracy: 0.7275\n",
            "Epoch 140/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.8298 - accuracy: 0.7336\n",
            "Epoch 141/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.7896 - accuracy: 0.7377\n",
            "Epoch 142/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.7398 - accuracy: 0.7664\n",
            "Epoch 143/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7478 - accuracy: 0.7561\n",
            "Epoch 144/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.7568 - accuracy: 0.7213\n",
            "Epoch 145/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.6947 - accuracy: 0.7459\n",
            "Epoch 146/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.7547 - accuracy: 0.7377\n",
            "Epoch 147/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8312 - accuracy: 0.7049\n",
            "Epoch 148/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.8378 - accuracy: 0.7295\n",
            "Epoch 149/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8305 - accuracy: 0.7254\n",
            "Epoch 150/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.8652 - accuracy: 0.7213\n",
            "Epoch 151/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.7825 - accuracy: 0.7541\n",
            "Epoch 152/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.7883 - accuracy: 0.7357\n",
            "Epoch 153/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8222 - accuracy: 0.7295\n",
            "Epoch 154/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.8540 - accuracy: 0.7152\n",
            "Epoch 155/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.8154 - accuracy: 0.7357\n",
            "Epoch 156/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.8954 - accuracy: 0.7049\n",
            "Epoch 157/250\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.8639 - accuracy: 0.7090\n",
            "Epoch 158/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 0.7986 - accuracy: 0.7254\n",
            "Epoch 159/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.7631 - accuracy: 0.7500\n",
            "Epoch 160/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.9019 - accuracy: 0.6885\n",
            "Epoch 161/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.7595 - accuracy: 0.7561\n",
            "Epoch 162/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7481 - accuracy: 0.7582\n",
            "Epoch 163/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.7624 - accuracy: 0.7275\n",
            "Epoch 164/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.7928 - accuracy: 0.7377\n",
            "Epoch 165/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.7006 - accuracy: 0.7541\n",
            "Epoch 166/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7689 - accuracy: 0.7234\n",
            "Epoch 167/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7090 - accuracy: 0.7480\n",
            "Epoch 168/250\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.7448 - accuracy: 0.7357\n",
            "Epoch 169/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8115 - accuracy: 0.7193\n",
            "Epoch 170/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.6713 - accuracy: 0.7623\n",
            "Epoch 171/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.8491 - accuracy: 0.7152\n",
            "Epoch 172/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.8268 - accuracy: 0.7336\n",
            "Epoch 173/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7384 - accuracy: 0.7398\n",
            "Epoch 174/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.8133 - accuracy: 0.7377\n",
            "Epoch 175/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.8304 - accuracy: 0.7029\n",
            "Epoch 176/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.8023 - accuracy: 0.7254\n",
            "Epoch 177/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.7211 - accuracy: 0.7480\n",
            "Epoch 178/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.7263 - accuracy: 0.7602\n",
            "Epoch 179/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.6966 - accuracy: 0.7684\n",
            "Epoch 180/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.6922 - accuracy: 0.7520\n",
            "Epoch 181/250\n",
            "16/16 [==============================] - 5s 328ms/step - loss: 0.7010 - accuracy: 0.7705\n",
            "Epoch 182/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.7118 - accuracy: 0.7295\n",
            "Epoch 183/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.6871 - accuracy: 0.7377\n",
            "Epoch 184/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6560 - accuracy: 0.7787\n",
            "Epoch 185/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.6601 - accuracy: 0.7848\n",
            "Epoch 186/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6850 - accuracy: 0.7602\n",
            "Epoch 187/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.7202 - accuracy: 0.7480\n",
            "Epoch 188/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.6727 - accuracy: 0.7787\n",
            "Epoch 189/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6872 - accuracy: 0.7602\n",
            "Epoch 190/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.7946 - accuracy: 0.7357\n",
            "Epoch 191/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.7764 - accuracy: 0.7582\n",
            "Epoch 192/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.7443 - accuracy: 0.7500\n",
            "Epoch 193/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7599 - accuracy: 0.7643\n",
            "Epoch 194/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.6983 - accuracy: 0.7480\n",
            "Epoch 195/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 0.7574 - accuracy: 0.7377\n",
            "Epoch 196/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7950 - accuracy: 0.7602\n",
            "Epoch 197/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.6984 - accuracy: 0.7541\n",
            "Epoch 198/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.7140 - accuracy: 0.7561\n",
            "Epoch 199/250\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 0.6270 - accuracy: 0.7889\n",
            "Epoch 200/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.7063 - accuracy: 0.7623\n",
            "Epoch 201/250\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 0.9123 - accuracy: 0.7193\n",
            "Epoch 202/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.7813 - accuracy: 0.7008\n",
            "Epoch 203/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.7275 - accuracy: 0.7602\n",
            "Epoch 204/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.6788 - accuracy: 0.7602\n",
            "Epoch 205/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.6526 - accuracy: 0.7705\n",
            "Epoch 206/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.6604 - accuracy: 0.7869\n",
            "Epoch 207/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.6547 - accuracy: 0.7684\n",
            "Epoch 208/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.6444 - accuracy: 0.7643\n",
            "Epoch 209/250\n",
            "16/16 [==============================] - 5s 324ms/step - loss: 0.7046 - accuracy: 0.7582\n",
            "Epoch 210/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.7007 - accuracy: 0.7398\n",
            "Epoch 211/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.6084 - accuracy: 0.7869\n",
            "Epoch 212/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6558 - accuracy: 0.7582\n",
            "Epoch 213/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 0.6483 - accuracy: 0.7910\n",
            "Epoch 214/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.6736 - accuracy: 0.7951\n",
            "Epoch 215/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.6260 - accuracy: 0.7992\n",
            "Epoch 216/250\n",
            "16/16 [==============================] - 5s 323ms/step - loss: 0.6220 - accuracy: 0.7910\n",
            "Epoch 217/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.5944 - accuracy: 0.7848\n",
            "Epoch 218/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.6348 - accuracy: 0.7869\n",
            "Epoch 219/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.6740 - accuracy: 0.7664\n",
            "Epoch 220/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.5854 - accuracy: 0.8156\n",
            "Epoch 221/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.5697 - accuracy: 0.8135\n",
            "Epoch 222/250\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.7390 - accuracy: 0.7459\n",
            "Epoch 223/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6579 - accuracy: 0.7602\n",
            "Epoch 224/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5750 - accuracy: 0.8115\n",
            "Epoch 225/250\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 0.6284 - accuracy: 0.7930\n",
            "Epoch 226/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.6159 - accuracy: 0.8033\n",
            "Epoch 227/250\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5859 - accuracy: 0.8053\n",
            "Epoch 228/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.6500 - accuracy: 0.7623\n",
            "Epoch 229/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.7050 - accuracy: 0.7705\n",
            "Epoch 230/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.7849 - accuracy: 0.7172\n",
            "Epoch 231/250\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.6740 - accuracy: 0.7561\n",
            "Epoch 232/250\n",
            "16/16 [==============================] - 5s 329ms/step - loss: 0.6537 - accuracy: 0.7971\n",
            "Epoch 233/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.6788 - accuracy: 0.7541\n",
            "Epoch 234/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6062 - accuracy: 0.7766\n",
            "Epoch 235/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6802 - accuracy: 0.7582\n",
            "Epoch 236/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.6014 - accuracy: 0.8012\n",
            "Epoch 237/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.6654 - accuracy: 0.7766\n",
            "Epoch 238/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.6211 - accuracy: 0.7869\n",
            "Epoch 239/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.6771 - accuracy: 0.7705\n",
            "Epoch 240/250\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.5941 - accuracy: 0.7869\n",
            "Epoch 241/250\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.6025 - accuracy: 0.8053\n",
            "Epoch 242/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6159 - accuracy: 0.7725\n",
            "Epoch 243/250\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5965 - accuracy: 0.7869\n",
            "Epoch 244/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.5856 - accuracy: 0.7807\n",
            "Epoch 245/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.6940 - accuracy: 0.7520\n",
            "Epoch 246/250\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.6342 - accuracy: 0.7910\n",
            "Epoch 247/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.5611 - accuracy: 0.8012\n",
            "Epoch 248/250\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.5909 - accuracy: 0.7828\n",
            "Epoch 249/250\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.5225 - accuracy: 0.8135\n",
            "Epoch 250/250\n",
            "16/16 [==============================] - 5s 321ms/step - loss: 0.6652 - accuracy: 0.7848\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1696d4d550>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShEDQPE_8q4m",
        "outputId": "331e72b5-819c-4d96-8a72-e81904ceb236"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 27ms/step - loss: 3.0895 - accuracy: 0.6341\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.089541435241699, 0.6341463327407837]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 모델 한줄빠진거\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model2 = models.Sequential()\n",
        "\n",
        "model2.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(224,224,3)))\n",
        "model2.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(layers.Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(layers.Flatten()) \n",
        "model2.add(layers.Dense(len(folder_name),activation=\"softmax\"))\n",
        "\n",
        "model2.summary()\n",
        "    \n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FijdN3HlFV_Y",
        "outputId": "33f1498a-ac1d-4f44-8c55-e3684e1945e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_13 (Conv2D)          (None, 222, 222, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 220, 220, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 110, 110, 64)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 108, 108, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 106, 106, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 53, 53, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 51, 51, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 25, 25, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,475,340\n",
            "Trainable params: 2,475,340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기존 모델로 돌리기(위에서 기존 모델이 정확도가 더 높았기 때문)\n",
        "#과적합 테스트\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model2.fit(train_gen, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqLdk5NQFdQ5",
        "outputId": "b9db9eff-c235-44c9-d3d6-1f8c2f6cb52f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "16/16 [==============================] - 12s 458ms/step - loss: 2.6446 - accuracy: 0.1107\n",
            "Epoch 2/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 2.3039 - accuracy: 0.1885\n",
            "Epoch 3/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 2.2284 - accuracy: 0.2172\n",
            "Epoch 4/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 2.1022 - accuracy: 0.2848\n",
            "Epoch 5/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 2.0083 - accuracy: 0.2664\n",
            "Epoch 6/250\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 1.9237 - accuracy: 0.3320\n",
            "Epoch 7/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.9120 - accuracy: 0.3381\n",
            "Epoch 8/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.7543 - accuracy: 0.3893\n",
            "Epoch 9/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.7721 - accuracy: 0.4016\n",
            "Epoch 10/250\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 1.7718 - accuracy: 0.3852\n",
            "Epoch 11/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.6600 - accuracy: 0.4365\n",
            "Epoch 12/250\n",
            "16/16 [==============================] - 7s 436ms/step - loss: 1.6364 - accuracy: 0.4385\n",
            "Epoch 13/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.6515 - accuracy: 0.4283\n",
            "Epoch 14/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.5727 - accuracy: 0.4529\n",
            "Epoch 15/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.6914 - accuracy: 0.4242\n",
            "Epoch 16/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 1.5129 - accuracy: 0.4754\n",
            "Epoch 17/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.4840 - accuracy: 0.5164\n",
            "Epoch 18/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.6144 - accuracy: 0.4508\n",
            "Epoch 19/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.4949 - accuracy: 0.4652\n",
            "Epoch 20/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.5143 - accuracy: 0.4693\n",
            "Epoch 21/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.4674 - accuracy: 0.4877\n",
            "Epoch 22/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.4244 - accuracy: 0.5061\n",
            "Epoch 23/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.4802 - accuracy: 0.4857\n",
            "Epoch 24/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.5983 - accuracy: 0.4344\n",
            "Epoch 25/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.6196 - accuracy: 0.4631\n",
            "Epoch 26/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.4231 - accuracy: 0.5410\n",
            "Epoch 27/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.4920 - accuracy: 0.4652\n",
            "Epoch 28/250\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 1.4629 - accuracy: 0.4877\n",
            "Epoch 29/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.4023 - accuracy: 0.5020\n",
            "Epoch 30/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.3044 - accuracy: 0.5430\n",
            "Epoch 31/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2972 - accuracy: 0.5594\n",
            "Epoch 32/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.3993 - accuracy: 0.5102\n",
            "Epoch 33/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.3669 - accuracy: 0.5205\n",
            "Epoch 34/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.3692 - accuracy: 0.5328\n",
            "Epoch 35/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.3471 - accuracy: 0.5164\n",
            "Epoch 36/250\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 1.3599 - accuracy: 0.5512\n",
            "Epoch 37/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.2895 - accuracy: 0.5656\n",
            "Epoch 38/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.4583 - accuracy: 0.5246\n",
            "Epoch 39/250\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 1.2587 - accuracy: 0.5225\n",
            "Epoch 40/250\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.3021 - accuracy: 0.5553\n",
            "Epoch 41/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2300 - accuracy: 0.5799\n",
            "Epoch 42/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.3171 - accuracy: 0.5594\n",
            "Epoch 43/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.2835 - accuracy: 0.5881\n",
            "Epoch 44/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.2234 - accuracy: 0.5943\n",
            "Epoch 45/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.1535 - accuracy: 0.6086\n",
            "Epoch 46/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.1779 - accuracy: 0.6107\n",
            "Epoch 47/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 1.0609 - accuracy: 0.6168\n",
            "Epoch 48/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.1658 - accuracy: 0.6148\n",
            "Epoch 49/250\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 1.0906 - accuracy: 0.6373\n",
            "Epoch 50/250\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 1.0157 - accuracy: 0.6496\n",
            "Epoch 51/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0978 - accuracy: 0.6352\n",
            "Epoch 52/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0922 - accuracy: 0.6270\n",
            "Epoch 53/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.0283 - accuracy: 0.6414\n",
            "Epoch 54/250\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.0789 - accuracy: 0.6516\n",
            "Epoch 55/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 1.0568 - accuracy: 0.6537\n",
            "Epoch 56/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.0081 - accuracy: 0.6250\n",
            "Epoch 57/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.9860 - accuracy: 0.6639\n",
            "Epoch 58/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.1445 - accuracy: 0.6168\n",
            "Epoch 59/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.2269 - accuracy: 0.6086\n",
            "Epoch 60/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 1.1694 - accuracy: 0.6168\n",
            "Epoch 61/250\n",
            "16/16 [==============================] - 6s 404ms/step - loss: 0.9608 - accuracy: 0.6762\n",
            "Epoch 62/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.0804 - accuracy: 0.6414\n",
            "Epoch 63/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.0720 - accuracy: 0.6107\n",
            "Epoch 64/250\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.9926 - accuracy: 0.6824\n",
            "Epoch 65/250\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.9937 - accuracy: 0.6619\n",
            "Epoch 66/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 1.3710 - accuracy: 0.5820\n",
            "Epoch 67/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 1.4729 - accuracy: 0.4898\n",
            "Epoch 68/250\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 1.3083 - accuracy: 0.5840\n",
            "Epoch 69/250\n",
            "16/16 [==============================] - 6s 401ms/step - loss: 1.2038 - accuracy: 0.5840\n",
            "Epoch 70/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 1.0540 - accuracy: 0.6332\n",
            "Epoch 71/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 1.0154 - accuracy: 0.6496\n",
            "Epoch 72/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9863 - accuracy: 0.6824\n",
            "Epoch 73/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.9318 - accuracy: 0.7090\n",
            "Epoch 74/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9460 - accuracy: 0.6865\n",
            "Epoch 75/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.9673 - accuracy: 0.6865\n",
            "Epoch 76/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.9514 - accuracy: 0.6885\n",
            "Epoch 77/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 1.0281 - accuracy: 0.6393\n",
            "Epoch 78/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.9176 - accuracy: 0.7131\n",
            "Epoch 79/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.9272 - accuracy: 0.6988\n",
            "Epoch 80/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.8323 - accuracy: 0.6824\n",
            "Epoch 81/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.1110 - accuracy: 0.6475\n",
            "Epoch 82/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.9083 - accuracy: 0.6824\n",
            "Epoch 83/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.8719 - accuracy: 0.6906\n",
            "Epoch 84/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.9281 - accuracy: 0.6988\n",
            "Epoch 85/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.7747 - accuracy: 0.7316\n",
            "Epoch 86/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.7923 - accuracy: 0.7398\n",
            "Epoch 87/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.7543 - accuracy: 0.7664\n",
            "Epoch 88/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.6797 - accuracy: 0.7705\n",
            "Epoch 89/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6956 - accuracy: 0.7623\n",
            "Epoch 90/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.7364 - accuracy: 0.7643\n",
            "Epoch 91/250\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.7550 - accuracy: 0.7582\n",
            "Epoch 92/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.7940 - accuracy: 0.7541\n",
            "Epoch 93/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.6986 - accuracy: 0.7725\n",
            "Epoch 94/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6607 - accuracy: 0.7602\n",
            "Epoch 95/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6493 - accuracy: 0.7828\n",
            "Epoch 96/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.7180 - accuracy: 0.7746\n",
            "Epoch 97/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6353 - accuracy: 0.7869\n",
            "Epoch 98/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6815 - accuracy: 0.7807\n",
            "Epoch 99/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.7191 - accuracy: 0.7787\n",
            "Epoch 100/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6285 - accuracy: 0.7725\n",
            "Epoch 101/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.9702 - accuracy: 0.6885\n",
            "Epoch 102/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 1.0744 - accuracy: 0.6496\n",
            "Epoch 103/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.9621 - accuracy: 0.6557\n",
            "Epoch 104/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.9569 - accuracy: 0.6947\n",
            "Epoch 105/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.8765 - accuracy: 0.7275\n",
            "Epoch 106/250\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.8552 - accuracy: 0.7561\n",
            "Epoch 107/250\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.8335 - accuracy: 0.7152\n",
            "Epoch 108/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.7357 - accuracy: 0.7459\n",
            "Epoch 109/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.7143 - accuracy: 0.7889\n",
            "Epoch 110/250\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6880 - accuracy: 0.7561\n",
            "Epoch 111/250\n",
            "16/16 [==============================] - 6s 409ms/step - loss: 0.7764 - accuracy: 0.7398\n",
            "Epoch 112/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6730 - accuracy: 0.7787\n",
            "Epoch 113/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.8620 - accuracy: 0.7439\n",
            "Epoch 114/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.7034 - accuracy: 0.7725\n",
            "Epoch 115/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5840 - accuracy: 0.8197\n",
            "Epoch 116/250\n",
            "16/16 [==============================] - 6s 409ms/step - loss: 0.6941 - accuracy: 0.7602\n",
            "Epoch 117/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6364 - accuracy: 0.8012\n",
            "Epoch 118/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.6584 - accuracy: 0.7746\n",
            "Epoch 119/250\n",
            "16/16 [==============================] - 6s 401ms/step - loss: 0.7330 - accuracy: 0.7664\n",
            "Epoch 120/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.6712 - accuracy: 0.7930\n",
            "Epoch 121/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5639 - accuracy: 0.7869\n",
            "Epoch 122/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6233 - accuracy: 0.8320\n",
            "Epoch 123/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5606 - accuracy: 0.8238\n",
            "Epoch 124/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4991 - accuracy: 0.8422\n",
            "Epoch 125/250\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 0.4696 - accuracy: 0.8402\n",
            "Epoch 126/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5848 - accuracy: 0.8279\n",
            "Epoch 127/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5387 - accuracy: 0.8115\n",
            "Epoch 128/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.8962 - accuracy: 0.7377\n",
            "Epoch 129/250\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.6644 - accuracy: 0.8074\n",
            "Epoch 130/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6123 - accuracy: 0.8012\n",
            "Epoch 131/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5452 - accuracy: 0.8299\n",
            "Epoch 132/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5198 - accuracy: 0.8217\n",
            "Epoch 133/250\n",
            "16/16 [==============================] - 6s 401ms/step - loss: 0.4742 - accuracy: 0.8484\n",
            "Epoch 134/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5238 - accuracy: 0.8299\n",
            "Epoch 135/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5188 - accuracy: 0.8299\n",
            "Epoch 136/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.4803 - accuracy: 0.8258\n",
            "Epoch 137/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4657 - accuracy: 0.8484\n",
            "Epoch 138/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4922 - accuracy: 0.8791\n",
            "Epoch 139/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4721 - accuracy: 0.8484\n",
            "Epoch 140/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4733 - accuracy: 0.8463\n",
            "Epoch 141/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.4794 - accuracy: 0.8607\n",
            "Epoch 142/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4228 - accuracy: 0.8689\n",
            "Epoch 143/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.3646 - accuracy: 0.8648\n",
            "Epoch 144/250\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.3769 - accuracy: 0.8873\n",
            "Epoch 145/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.4189 - accuracy: 0.8730\n",
            "Epoch 146/250\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.3803 - accuracy: 0.8852\n",
            "Epoch 147/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3591 - accuracy: 0.8832\n",
            "Epoch 148/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.3500 - accuracy: 0.8811\n",
            "Epoch 149/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3898 - accuracy: 0.8709\n",
            "Epoch 150/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3785 - accuracy: 0.8730\n",
            "Epoch 151/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3709 - accuracy: 0.8852\n",
            "Epoch 152/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.2886 - accuracy: 0.9037\n",
            "Epoch 153/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.3154 - accuracy: 0.8934\n",
            "Epoch 154/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.2871 - accuracy: 0.8934\n",
            "Epoch 155/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3108 - accuracy: 0.8955\n",
            "Epoch 156/250\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 0.3997 - accuracy: 0.8893\n",
            "Epoch 157/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.4840 - accuracy: 0.8463\n",
            "Epoch 158/250\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.4002 - accuracy: 0.8811\n",
            "Epoch 159/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4867 - accuracy: 0.8504\n",
            "Epoch 160/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.4075 - accuracy: 0.8750\n",
            "Epoch 161/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.4267 - accuracy: 0.8770\n",
            "Epoch 162/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3627 - accuracy: 0.8709\n",
            "Epoch 163/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.3693 - accuracy: 0.8914\n",
            "Epoch 164/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.2982 - accuracy: 0.9078\n",
            "Epoch 165/250\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.3813 - accuracy: 0.8893\n",
            "Epoch 166/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3402 - accuracy: 0.8832\n",
            "Epoch 167/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.2705 - accuracy: 0.9201\n",
            "Epoch 168/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2093 - accuracy: 0.9365\n",
            "Epoch 169/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2012 - accuracy: 0.9467\n",
            "Epoch 170/250\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.2358 - accuracy: 0.9180\n",
            "Epoch 171/250\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.2242 - accuracy: 0.9262\n",
            "Epoch 172/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.2401 - accuracy: 0.9303\n",
            "Epoch 173/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2747 - accuracy: 0.9160\n",
            "Epoch 174/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3170 - accuracy: 0.9016\n",
            "Epoch 175/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.3289 - accuracy: 0.8955\n",
            "Epoch 176/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.3820 - accuracy: 0.8955\n",
            "Epoch 177/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2586 - accuracy: 0.9139\n",
            "Epoch 178/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.2644 - accuracy: 0.9180\n",
            "Epoch 179/250\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.1701 - accuracy: 0.9406\n",
            "Epoch 180/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.3323 - accuracy: 0.9119\n",
            "Epoch 181/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.3357 - accuracy: 0.9098\n",
            "Epoch 182/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2506 - accuracy: 0.9221\n",
            "Epoch 183/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2458 - accuracy: 0.9221\n",
            "Epoch 184/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.2262 - accuracy: 0.9221\n",
            "Epoch 185/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.4324 - accuracy: 0.8914\n",
            "Epoch 186/250\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.9500 - accuracy: 0.7746\n",
            "Epoch 187/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5916 - accuracy: 0.8320\n",
            "Epoch 188/250\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.4175 - accuracy: 0.8545\n",
            "Epoch 189/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.3523 - accuracy: 0.8811\n",
            "Epoch 190/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.4446 - accuracy: 0.8648\n",
            "Epoch 191/250\n",
            "16/16 [==============================] - 6s 410ms/step - loss: 0.3929 - accuracy: 0.9037\n",
            "Epoch 192/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.3743 - accuracy: 0.8852\n",
            "Epoch 193/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.2854 - accuracy: 0.8893\n",
            "Epoch 194/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.2836 - accuracy: 0.9201\n",
            "Epoch 195/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.4017 - accuracy: 0.8832\n",
            "Epoch 196/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.2563 - accuracy: 0.9098\n",
            "Epoch 197/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.2356 - accuracy: 0.9324\n",
            "Epoch 198/250\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.3173 - accuracy: 0.9098\n",
            "Epoch 199/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.2655 - accuracy: 0.9160\n",
            "Epoch 200/250\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.1938 - accuracy: 0.9447\n",
            "Epoch 201/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.2771 - accuracy: 0.9037\n",
            "Epoch 202/250\n",
            "16/16 [==============================] - 6s 402ms/step - loss: 0.2399 - accuracy: 0.9201\n",
            "Epoch 203/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2416 - accuracy: 0.9242\n",
            "Epoch 204/250\n",
            "16/16 [==============================] - 6s 398ms/step - loss: 0.1974 - accuracy: 0.9242\n",
            "Epoch 205/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.1980 - accuracy: 0.9262\n",
            "Epoch 206/250\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.1685 - accuracy: 0.9406\n",
            "Epoch 207/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.1701 - accuracy: 0.9385\n",
            "Epoch 208/250\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.2089 - accuracy: 0.9406\n",
            "Epoch 209/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2924 - accuracy: 0.9119\n",
            "Epoch 210/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.2197 - accuracy: 0.9406\n",
            "Epoch 211/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2069 - accuracy: 0.9303\n",
            "Epoch 212/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.1969 - accuracy: 0.9488\n",
            "Epoch 213/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2253 - accuracy: 0.9262\n",
            "Epoch 214/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.1673 - accuracy: 0.9426\n",
            "Epoch 215/250\n",
            "16/16 [==============================] - 6s 403ms/step - loss: 0.2700 - accuracy: 0.9242\n",
            "Epoch 216/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2391 - accuracy: 0.9201\n",
            "Epoch 217/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2000 - accuracy: 0.9467\n",
            "Epoch 218/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.1356 - accuracy: 0.9590\n",
            "Epoch 219/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.1211 - accuracy: 0.9590\n",
            "Epoch 220/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.1372 - accuracy: 0.9631\n",
            "Epoch 221/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.1528 - accuracy: 0.9529\n",
            "Epoch 222/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.2792 - accuracy: 0.9098\n",
            "Epoch 223/250\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.3016 - accuracy: 0.9057\n",
            "Epoch 224/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2787 - accuracy: 0.9201\n",
            "Epoch 225/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.1953 - accuracy: 0.9385\n",
            "Epoch 226/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.3017 - accuracy: 0.9221\n",
            "Epoch 227/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.2743 - accuracy: 0.9221\n",
            "Epoch 228/250\n",
            "16/16 [==============================] - 6s 404ms/step - loss: 0.3954 - accuracy: 0.8996\n",
            "Epoch 229/250\n",
            "16/16 [==============================] - 6s 404ms/step - loss: 0.3103 - accuracy: 0.9119\n",
            "Epoch 230/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.2245 - accuracy: 0.9324\n",
            "Epoch 231/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.1988 - accuracy: 0.9221\n",
            "Epoch 232/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.2604 - accuracy: 0.9221\n",
            "Epoch 233/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.2074 - accuracy: 0.9488\n",
            "Epoch 234/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.2347 - accuracy: 0.9119\n",
            "Epoch 235/250\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.2054 - accuracy: 0.9467\n",
            "Epoch 236/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.2319 - accuracy: 0.9385\n",
            "Epoch 237/250\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.1545 - accuracy: 0.9508\n",
            "Epoch 238/250\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.1697 - accuracy: 0.9406\n",
            "Epoch 239/250\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.1643 - accuracy: 0.9385\n",
            "Epoch 240/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.1708 - accuracy: 0.9508\n",
            "Epoch 241/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.2291 - accuracy: 0.9242\n",
            "Epoch 242/250\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.1590 - accuracy: 0.9590\n",
            "Epoch 243/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.1978 - accuracy: 0.9303\n",
            "Epoch 244/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.2259 - accuracy: 0.9447\n",
            "Epoch 245/250\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.2076 - accuracy: 0.9344\n",
            "Epoch 246/250\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.2231 - accuracy: 0.9283\n",
            "Epoch 247/250\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.1453 - accuracy: 0.9467\n",
            "Epoch 248/250\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.1702 - accuracy: 0.9549\n",
            "Epoch 249/250\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.1290 - accuracy: 0.9611\n",
            "Epoch 250/250\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.2103 - accuracy: 0.9385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1696280950>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE1VV3ZcFff6",
        "outputId": "192e39ff-bb4c-446d-cfa0-214ffee9aaa2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 2s 573ms/step - loss: 2.6481 - accuracy: 0.7236\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6480915546417236, 0.7235772609710693]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# l2 모델\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "model5 = models.Sequential()\n",
        "\n",
        "model5.add(layers.Conv2D(32, (3, 3),input_shape=X_train.shape[1:],\n",
        "                         kernel_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(layers.Activation('relu'))\n",
        "model5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model5.add(layers.Dropout(0.4))\n",
        "\n",
        "model5.add(layers.Conv2D(32, (3, 3),  kernel_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(layers.Activation('relu'))\n",
        "model5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model5.add(layers.Conv2D(64, (3, 3),  kernel_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(layers.Activation('relu'))\n",
        "model5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model5.add(layers.Dropout(0.5))\n",
        "\n",
        "model5.add(layers.Flatten()) # Output convert into one dimension layer and will go to Dense layer\n",
        "model5.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001)))\n",
        "model5.add(layers.Activation('relu'))\n",
        "model5.add(layers.Dropout(0.4))\n",
        "model5.add(layers.Dense(12,  kernel_regularizer=regularizers.l2(0.001)))\n",
        "model5.add(layers.Activation('softmax'))\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy', \n",
        "            optimizer=\"adam\",\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtSmI1JuOP8O",
        "outputId": "4fd279ad-b97b-4d9a-c53c-ab1d2a409833"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_31 (Conv2D)          (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 222, 222, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  (None, 111, 111, 32)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 111, 111, 32)      0         \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 107, 107, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  (None, 53, 53, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 51, 51, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 49, 49, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  (None, 24, 24, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 73728)             0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 64)                4718656   \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 12)                780       \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 12)                156       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,997,352\n",
            "Trainable params: 4,997,352\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#기존 모델로 돌리기(위에서 기존 모델이 정확도가 더 높았기 때문)\n",
        "#과적합 테스트\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=32)\n",
        "\n",
        "model5.fit(train_gen, epochs=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0jqlha2BO5Tu",
        "outputId": "d86d122f-5f22-41a1-e86a-95a517490f3f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "16/16 [==============================] - 6s 307ms/step - loss: 3.1004 - accuracy: 0.1004\n",
            "Epoch 2/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 2.6937 - accuracy: 0.1066\n",
            "Epoch 3/250\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 2.6818 - accuracy: 0.0963\n",
            "Epoch 4/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.6148 - accuracy: 0.0861\n",
            "Epoch 5/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 2.5066 - accuracy: 0.1045\n",
            "Epoch 6/250\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 2.5000 - accuracy: 0.1434\n",
            "Epoch 7/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 2.4672 - accuracy: 0.1311\n",
            "Epoch 8/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.4377 - accuracy: 0.1639\n",
            "Epoch 9/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.3943 - accuracy: 0.1803\n",
            "Epoch 10/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.3597 - accuracy: 0.2131\n",
            "Epoch 11/250\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 2.3371 - accuracy: 0.2193\n",
            "Epoch 12/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.3446 - accuracy: 0.2193\n",
            "Epoch 13/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.2900 - accuracy: 0.2316\n",
            "Epoch 14/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 2.2687 - accuracy: 0.2418\n",
            "Epoch 15/250\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 2.2434 - accuracy: 0.2439\n",
            "Epoch 16/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.3056 - accuracy: 0.2377\n",
            "Epoch 17/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.2531 - accuracy: 0.2500\n",
            "Epoch 18/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 2.2235 - accuracy: 0.2602\n",
            "Epoch 19/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.1929 - accuracy: 0.2500\n",
            "Epoch 20/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 2.2061 - accuracy: 0.2643\n",
            "Epoch 21/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.2212 - accuracy: 0.2398\n",
            "Epoch 22/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 2.2134 - accuracy: 0.2561\n",
            "Epoch 23/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 2.1760 - accuracy: 0.2889\n",
            "Epoch 24/250\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 2.1595 - accuracy: 0.2910\n",
            "Epoch 25/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.1645 - accuracy: 0.2725\n",
            "Epoch 26/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.1258 - accuracy: 0.2971\n",
            "Epoch 27/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.1757 - accuracy: 0.2930\n",
            "Epoch 28/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 2.1260 - accuracy: 0.2910\n",
            "Epoch 29/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 2.0970 - accuracy: 0.2725\n",
            "Epoch 30/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.0941 - accuracy: 0.2807\n",
            "Epoch 31/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 2.0723 - accuracy: 0.3033\n",
            "Epoch 32/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.0521 - accuracy: 0.2807\n",
            "Epoch 33/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.0950 - accuracy: 0.2807\n",
            "Epoch 34/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.0339 - accuracy: 0.3443\n",
            "Epoch 35/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 2.0817 - accuracy: 0.3238\n",
            "Epoch 36/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 2.0430 - accuracy: 0.2971\n",
            "Epoch 37/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 2.0383 - accuracy: 0.3381\n",
            "Epoch 38/250\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 2.0192 - accuracy: 0.3340\n",
            "Epoch 39/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 2.0155 - accuracy: 0.3012\n",
            "Epoch 40/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9751 - accuracy: 0.2992\n",
            "Epoch 41/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9646 - accuracy: 0.3463\n",
            "Epoch 42/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.9697 - accuracy: 0.3299\n",
            "Epoch 43/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9992 - accuracy: 0.3566\n",
            "Epoch 44/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9754 - accuracy: 0.3299\n",
            "Epoch 45/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.9282 - accuracy: 0.3545\n",
            "Epoch 46/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.9471 - accuracy: 0.3607\n",
            "Epoch 47/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.9639 - accuracy: 0.3279\n",
            "Epoch 48/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.9610 - accuracy: 0.3463\n",
            "Epoch 49/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.9103 - accuracy: 0.3381\n",
            "Epoch 50/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.9816 - accuracy: 0.3402\n",
            "Epoch 51/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9620 - accuracy: 0.3443\n",
            "Epoch 52/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8968 - accuracy: 0.3730\n",
            "Epoch 53/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9348 - accuracy: 0.3422\n",
            "Epoch 54/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.9515 - accuracy: 0.3197\n",
            "Epoch 55/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.8670 - accuracy: 0.3545\n",
            "Epoch 56/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.8878 - accuracy: 0.3730\n",
            "Epoch 57/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.9675 - accuracy: 0.3852\n",
            "Epoch 58/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8747 - accuracy: 0.3811\n",
            "Epoch 59/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.9148 - accuracy: 0.3607\n",
            "Epoch 60/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8836 - accuracy: 0.3811\n",
            "Epoch 61/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.8686 - accuracy: 0.3873\n",
            "Epoch 62/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.9686 - accuracy: 0.3504\n",
            "Epoch 63/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8843 - accuracy: 0.3586\n",
            "Epoch 64/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.9400 - accuracy: 0.3422\n",
            "Epoch 65/250\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 1.8826 - accuracy: 0.3607\n",
            "Epoch 66/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8689 - accuracy: 0.3422\n",
            "Epoch 67/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.9059 - accuracy: 0.3709\n",
            "Epoch 68/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8973 - accuracy: 0.3730\n",
            "Epoch 69/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8464 - accuracy: 0.3586\n",
            "Epoch 70/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.9168 - accuracy: 0.3566\n",
            "Epoch 71/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8148 - accuracy: 0.3811\n",
            "Epoch 72/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8261 - accuracy: 0.3893\n",
            "Epoch 73/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8570 - accuracy: 0.3893\n",
            "Epoch 74/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.8439 - accuracy: 0.3709\n",
            "Epoch 75/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8618 - accuracy: 0.3668\n",
            "Epoch 76/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7805 - accuracy: 0.3873\n",
            "Epoch 77/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8840 - accuracy: 0.3648\n",
            "Epoch 78/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8807 - accuracy: 0.4016\n",
            "Epoch 79/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.8435 - accuracy: 0.3750\n",
            "Epoch 80/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8307 - accuracy: 0.3750\n",
            "Epoch 81/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8336 - accuracy: 0.4078\n",
            "Epoch 82/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8376 - accuracy: 0.3791\n",
            "Epoch 83/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7678 - accuracy: 0.3975\n",
            "Epoch 84/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.8661 - accuracy: 0.3750\n",
            "Epoch 85/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.8288 - accuracy: 0.3914\n",
            "Epoch 86/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8509 - accuracy: 0.3996\n",
            "Epoch 87/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8193 - accuracy: 0.3914\n",
            "Epoch 88/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8576 - accuracy: 0.4098\n",
            "Epoch 89/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.8165 - accuracy: 0.4057\n",
            "Epoch 90/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8219 - accuracy: 0.3914\n",
            "Epoch 91/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.8214 - accuracy: 0.3955\n",
            "Epoch 92/250\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 1.8735 - accuracy: 0.3648\n",
            "Epoch 93/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.7743 - accuracy: 0.3750\n",
            "Epoch 94/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8300 - accuracy: 0.4119\n",
            "Epoch 95/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8067 - accuracy: 0.4303\n",
            "Epoch 96/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8605 - accuracy: 0.3893\n",
            "Epoch 97/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.8045 - accuracy: 0.3791\n",
            "Epoch 98/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8516 - accuracy: 0.3893\n",
            "Epoch 99/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.8736 - accuracy: 0.3811\n",
            "Epoch 100/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.7834 - accuracy: 0.3873\n",
            "Epoch 101/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7693 - accuracy: 0.4242\n",
            "Epoch 102/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.7665 - accuracy: 0.3996\n",
            "Epoch 103/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8522 - accuracy: 0.3873\n",
            "Epoch 104/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7683 - accuracy: 0.4262\n",
            "Epoch 105/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8222 - accuracy: 0.3955\n",
            "Epoch 106/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7350 - accuracy: 0.4037\n",
            "Epoch 107/250\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 1.8161 - accuracy: 0.4160\n",
            "Epoch 108/250\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.8173 - accuracy: 0.4098\n",
            "Epoch 109/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.7843 - accuracy: 0.4221\n",
            "Epoch 110/250\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.7834 - accuracy: 0.4160\n",
            "Epoch 111/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8228 - accuracy: 0.4221\n",
            "Epoch 112/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7486 - accuracy: 0.4385\n",
            "Epoch 113/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7637 - accuracy: 0.4119\n",
            "Epoch 114/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.7682 - accuracy: 0.4037\n",
            "Epoch 115/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.8027 - accuracy: 0.4119\n",
            "Epoch 116/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.8389 - accuracy: 0.4037\n",
            "Epoch 117/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7383 - accuracy: 0.4057\n",
            "Epoch 118/250\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.7666 - accuracy: 0.4467\n",
            "Epoch 119/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.7809 - accuracy: 0.4037\n",
            "Epoch 120/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8161 - accuracy: 0.3975\n",
            "Epoch 121/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7318 - accuracy: 0.4447\n",
            "Epoch 122/250\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 1.7142 - accuracy: 0.4488\n",
            "Epoch 123/250\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 1.7978 - accuracy: 0.4242\n",
            "Epoch 124/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.7671 - accuracy: 0.4426\n",
            "Epoch 125/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7633 - accuracy: 0.4201\n",
            "Epoch 126/250\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 1.7712 - accuracy: 0.4119\n",
            "Epoch 127/250\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 1.7479 - accuracy: 0.4160\n",
            "Epoch 128/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8610 - accuracy: 0.3955\n",
            "Epoch 129/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.8599 - accuracy: 0.3955\n",
            "Epoch 130/250\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 1.7808 - accuracy: 0.4160\n",
            "Epoch 131/250\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.7756 - accuracy: 0.4180\n",
            "Epoch 132/250\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 1.7971 - accuracy: 0.4406\n",
            "Epoch 133/250\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 1.7993 - accuracy: 0.4344\n",
            "Epoch 134/250\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 1.7710 - accuracy: 0.4344\n",
            "Epoch 135/250\n",
            "11/16 [===================>..........] - ETA: 1s - loss: 1.6852 - accuracy: 0.4543"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-aa4cfed9582f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#내가 하고싶은 모델\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model7 = models.Sequential()\n",
        "\n",
        "model7.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(224,224,3)))\n",
        "model7.add(layers.Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
        "model7.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model7.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model7.add(layers.Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
        "model7.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model7.add(layers.Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model7.add(layers.Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
        "model7.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model7.add(layers.Flatten()) \n",
        "model7.add(layers.Dense(len(folder_name),activation=\"softmax\"))\n",
        "\n",
        "model7.summary()\n",
        "    \n",
        "model7.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm_iw3MFnzO2",
        "outputId": "2985a7ae-5a22-461a-eff4-a46e36eeed20"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_60 (Conv2D)          (None, 222, 222, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 220, 220, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_42 (MaxPoolin  (None, 110, 110, 64)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 108, 108, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_63 (Conv2D)          (None, 106, 106, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_43 (MaxPoolin  (None, 53, 53, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 51, 51, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_65 (Conv2D)          (None, 49, 49, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_44 (MaxPoolin  (None, 24, 24, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 147456)            0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 12)                1769484   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,914,892\n",
            "Trainable params: 2,914,892\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 돌리기\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(   \n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range = 0.3,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_gen = datagen.flow(X_train, y_train, batch_size=64)\n",
        "\n",
        "model7.fit(train_gen, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSihiBrVn_qb",
        "outputId": "822f8f93-b749-4d92-b490-974f46356392"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "8/8 [==============================] - 20s 1s/step - loss: 3.1867 - accuracy: 0.0881\n",
            "Epoch 2/150\n",
            "8/8 [==============================] - 6s 751ms/step - loss: 2.3830 - accuracy: 0.1352\n",
            "Epoch 3/150\n",
            "8/8 [==============================] - 6s 751ms/step - loss: 2.2714 - accuracy: 0.2008\n",
            "Epoch 4/150\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 2.2155 - accuracy: 0.2090\n",
            "Epoch 5/150\n",
            "8/8 [==============================] - 6s 747ms/step - loss: 2.0759 - accuracy: 0.2500\n",
            "Epoch 6/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 2.0869 - accuracy: 0.2848\n",
            "Epoch 7/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.9941 - accuracy: 0.3115\n",
            "Epoch 8/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 1.8418 - accuracy: 0.3504\n",
            "Epoch 9/150\n",
            "8/8 [==============================] - 6s 751ms/step - loss: 1.8048 - accuracy: 0.3443\n",
            "Epoch 10/150\n",
            "8/8 [==============================] - 6s 751ms/step - loss: 1.8067 - accuracy: 0.3811\n",
            "Epoch 11/150\n",
            "8/8 [==============================] - 6s 751ms/step - loss: 1.7276 - accuracy: 0.3873\n",
            "Epoch 12/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 1.7085 - accuracy: 0.3852\n",
            "Epoch 13/150\n",
            "8/8 [==============================] - 6s 754ms/step - loss: 1.6928 - accuracy: 0.4078\n",
            "Epoch 14/150\n",
            "8/8 [==============================] - 6s 760ms/step - loss: 1.7093 - accuracy: 0.4119\n",
            "Epoch 15/150\n",
            "8/8 [==============================] - 6s 750ms/step - loss: 1.6209 - accuracy: 0.4447\n",
            "Epoch 16/150\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 1.6318 - accuracy: 0.4549\n",
            "Epoch 17/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 1.6144 - accuracy: 0.4549\n",
            "Epoch 18/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.6222 - accuracy: 0.4385\n",
            "Epoch 19/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 1.5796 - accuracy: 0.4488\n",
            "Epoch 20/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 1.5383 - accuracy: 0.4836\n",
            "Epoch 21/150\n",
            "8/8 [==============================] - 6s 751ms/step - loss: 1.5572 - accuracy: 0.4754\n",
            "Epoch 22/150\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 1.5978 - accuracy: 0.4344\n",
            "Epoch 23/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 1.5722 - accuracy: 0.4693\n",
            "Epoch 24/150\n",
            "8/8 [==============================] - 6s 820ms/step - loss: 1.4565 - accuracy: 0.4939\n",
            "Epoch 25/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.4333 - accuracy: 0.5143\n",
            "Epoch 26/150\n",
            "8/8 [==============================] - 6s 823ms/step - loss: 1.4476 - accuracy: 0.4959\n",
            "Epoch 27/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 1.4514 - accuracy: 0.5041\n",
            "Epoch 28/150\n",
            "8/8 [==============================] - 6s 812ms/step - loss: 1.2954 - accuracy: 0.5430\n",
            "Epoch 29/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 1.4347 - accuracy: 0.4836\n",
            "Epoch 30/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 1.4755 - accuracy: 0.5082\n",
            "Epoch 31/150\n",
            "8/8 [==============================] - 6s 812ms/step - loss: 1.5094 - accuracy: 0.4754\n",
            "Epoch 32/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.4087 - accuracy: 0.5184\n",
            "Epoch 33/150\n",
            "8/8 [==============================] - 6s 815ms/step - loss: 1.4022 - accuracy: 0.5205\n",
            "Epoch 34/150\n",
            "8/8 [==============================] - 6s 760ms/step - loss: 1.3245 - accuracy: 0.5287\n",
            "Epoch 35/150\n",
            "8/8 [==============================] - 6s 744ms/step - loss: 1.3577 - accuracy: 0.5451\n",
            "Epoch 36/150\n",
            "8/8 [==============================] - 6s 811ms/step - loss: 1.3201 - accuracy: 0.5430\n",
            "Epoch 37/150\n",
            "8/8 [==============================] - 6s 818ms/step - loss: 1.3947 - accuracy: 0.5287\n",
            "Epoch 38/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 1.4294 - accuracy: 0.5102\n",
            "Epoch 39/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 1.3111 - accuracy: 0.5656\n",
            "Epoch 40/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 1.3104 - accuracy: 0.5369\n",
            "Epoch 41/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 1.1946 - accuracy: 0.5963\n",
            "Epoch 42/150\n",
            "8/8 [==============================] - 6s 741ms/step - loss: 1.2680 - accuracy: 0.5635\n",
            "Epoch 43/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 1.2767 - accuracy: 0.5738\n",
            "Epoch 44/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.2570 - accuracy: 0.5574\n",
            "Epoch 45/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 1.2631 - accuracy: 0.5881\n",
            "Epoch 46/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.1711 - accuracy: 0.6045\n",
            "Epoch 47/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 1.1368 - accuracy: 0.6311\n",
            "Epoch 48/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.1236 - accuracy: 0.6230\n",
            "Epoch 49/150\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 1.1102 - accuracy: 0.6066\n",
            "Epoch 50/150\n",
            "8/8 [==============================] - 6s 760ms/step - loss: 1.0179 - accuracy: 0.6701\n",
            "Epoch 51/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 1.0760 - accuracy: 0.6352\n",
            "Epoch 52/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 1.3570 - accuracy: 0.5594\n",
            "Epoch 53/150\n",
            "8/8 [==============================] - 6s 750ms/step - loss: 1.3778 - accuracy: 0.5082\n",
            "Epoch 54/150\n",
            "8/8 [==============================] - 6s 745ms/step - loss: 1.4673 - accuracy: 0.5471\n",
            "Epoch 55/150\n",
            "8/8 [==============================] - 6s 746ms/step - loss: 1.4030 - accuracy: 0.5225\n",
            "Epoch 56/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 1.2299 - accuracy: 0.5676\n",
            "Epoch 57/150\n",
            "8/8 [==============================] - 6s 746ms/step - loss: 1.1283 - accuracy: 0.6025\n",
            "Epoch 58/150\n",
            "8/8 [==============================] - 6s 766ms/step - loss: 1.0918 - accuracy: 0.6660\n",
            "Epoch 59/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 1.1628 - accuracy: 0.6066\n",
            "Epoch 60/150\n",
            "8/8 [==============================] - 6s 739ms/step - loss: 1.0475 - accuracy: 0.6352\n",
            "Epoch 61/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.9500 - accuracy: 0.6988\n",
            "Epoch 62/150\n",
            "8/8 [==============================] - 6s 761ms/step - loss: 0.9674 - accuracy: 0.6598\n",
            "Epoch 63/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 1.0501 - accuracy: 0.6230\n",
            "Epoch 64/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 1.0004 - accuracy: 0.6537\n",
            "Epoch 65/150\n",
            "8/8 [==============================] - 6s 754ms/step - loss: 0.9711 - accuracy: 0.6783\n",
            "Epoch 66/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 0.8727 - accuracy: 0.7439\n",
            "Epoch 67/150\n",
            "8/8 [==============================] - 6s 767ms/step - loss: 0.8731 - accuracy: 0.7295\n",
            "Epoch 68/150\n",
            "8/8 [==============================] - 6s 749ms/step - loss: 0.8284 - accuracy: 0.7234\n",
            "Epoch 69/150\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 0.8464 - accuracy: 0.7316\n",
            "Epoch 70/150\n",
            "8/8 [==============================] - 6s 742ms/step - loss: 0.8176 - accuracy: 0.7234\n",
            "Epoch 71/150\n",
            "8/8 [==============================] - 6s 820ms/step - loss: 0.8119 - accuracy: 0.7480\n",
            "Epoch 72/150\n",
            "8/8 [==============================] - 6s 749ms/step - loss: 0.8764 - accuracy: 0.7090\n",
            "Epoch 73/150\n",
            "8/8 [==============================] - 6s 816ms/step - loss: 0.7837 - accuracy: 0.7541\n",
            "Epoch 74/150\n",
            "8/8 [==============================] - 6s 761ms/step - loss: 0.8395 - accuracy: 0.7295\n",
            "Epoch 75/150\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 0.7514 - accuracy: 0.7602\n",
            "Epoch 76/150\n",
            "8/8 [==============================] - 6s 820ms/step - loss: 0.7983 - accuracy: 0.7500\n",
            "Epoch 77/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.7245 - accuracy: 0.7828\n",
            "Epoch 78/150\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 0.8467 - accuracy: 0.7418\n",
            "Epoch 79/150\n",
            "8/8 [==============================] - 6s 754ms/step - loss: 0.7419 - accuracy: 0.7643\n",
            "Epoch 80/150\n",
            "8/8 [==============================] - 6s 754ms/step - loss: 1.0687 - accuracy: 0.6721\n",
            "Epoch 81/150\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 1.1470 - accuracy: 0.6557\n",
            "Epoch 82/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 1.0453 - accuracy: 0.6680\n",
            "Epoch 83/150\n",
            "8/8 [==============================] - 6s 760ms/step - loss: 0.9312 - accuracy: 0.7049\n",
            "Epoch 84/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.7804 - accuracy: 0.7439\n",
            "Epoch 85/150\n",
            "8/8 [==============================] - 6s 741ms/step - loss: 0.7894 - accuracy: 0.7459\n",
            "Epoch 86/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.7550 - accuracy: 0.7561\n",
            "Epoch 87/150\n",
            "8/8 [==============================] - 6s 760ms/step - loss: 0.8550 - accuracy: 0.7213\n",
            "Epoch 88/150\n",
            "8/8 [==============================] - 6s 747ms/step - loss: 1.0459 - accuracy: 0.6721\n",
            "Epoch 89/150\n",
            "8/8 [==============================] - 6s 743ms/step - loss: 0.9009 - accuracy: 0.6988\n",
            "Epoch 90/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.8477 - accuracy: 0.7398\n",
            "Epoch 91/150\n",
            "8/8 [==============================] - 6s 747ms/step - loss: 0.6815 - accuracy: 0.7910\n",
            "Epoch 92/150\n",
            "8/8 [==============================] - 6s 735ms/step - loss: 0.9001 - accuracy: 0.7152\n",
            "Epoch 93/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 0.7733 - accuracy: 0.7582\n",
            "Epoch 94/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.7542 - accuracy: 0.7316\n",
            "Epoch 95/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 0.7340 - accuracy: 0.7500\n",
            "Epoch 96/150\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 0.6272 - accuracy: 0.7951\n",
            "Epoch 97/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.5954 - accuracy: 0.8033\n",
            "Epoch 98/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 0.6635 - accuracy: 0.8094\n",
            "Epoch 99/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.6611 - accuracy: 0.7992\n",
            "Epoch 100/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 0.6061 - accuracy: 0.8176\n",
            "Epoch 101/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.8344 - accuracy: 0.7684\n",
            "Epoch 102/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.8142 - accuracy: 0.7418\n",
            "Epoch 103/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 0.8448 - accuracy: 0.7541\n",
            "Epoch 104/150\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 0.6941 - accuracy: 0.7684\n",
            "Epoch 105/150\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 0.5428 - accuracy: 0.8320\n",
            "Epoch 106/150\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 0.6372 - accuracy: 0.8033\n",
            "Epoch 107/150\n",
            "8/8 [==============================] - 6s 749ms/step - loss: 0.4569 - accuracy: 0.8340\n",
            "Epoch 108/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 0.5805 - accuracy: 0.8135\n",
            "Epoch 109/150\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 0.4848 - accuracy: 0.8586\n",
            "Epoch 110/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 0.4418 - accuracy: 0.8709\n",
            "Epoch 111/150\n",
            "8/8 [==============================] - 6s 814ms/step - loss: 0.7583 - accuracy: 0.7807\n",
            "Epoch 112/150\n",
            "8/8 [==============================] - 6s 746ms/step - loss: 0.7657 - accuracy: 0.7480\n",
            "Epoch 113/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.7070 - accuracy: 0.7684\n",
            "Epoch 114/150\n",
            "8/8 [==============================] - 6s 749ms/step - loss: 0.6550 - accuracy: 0.7992\n",
            "Epoch 115/150\n",
            "8/8 [==============================] - 6s 770ms/step - loss: 0.6038 - accuracy: 0.8074\n",
            "Epoch 116/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 0.6268 - accuracy: 0.8094\n",
            "Epoch 117/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 0.5868 - accuracy: 0.8217\n",
            "Epoch 118/150\n",
            "8/8 [==============================] - 6s 822ms/step - loss: 0.4965 - accuracy: 0.8422\n",
            "Epoch 119/150\n",
            "8/8 [==============================] - 7s 770ms/step - loss: 0.4570 - accuracy: 0.8525\n",
            "Epoch 120/150\n",
            "8/8 [==============================] - 6s 761ms/step - loss: 0.4649 - accuracy: 0.8668\n",
            "Epoch 121/150\n",
            "8/8 [==============================] - 6s 761ms/step - loss: 0.4364 - accuracy: 0.8607\n",
            "Epoch 122/150\n",
            "8/8 [==============================] - 6s 756ms/step - loss: 0.3420 - accuracy: 0.8791\n",
            "Epoch 123/150\n",
            "8/8 [==============================] - 6s 760ms/step - loss: 0.5590 - accuracy: 0.8566\n",
            "Epoch 124/150\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 0.4496 - accuracy: 0.8443\n",
            "Epoch 125/150\n",
            "8/8 [==============================] - 6s 747ms/step - loss: 0.4402 - accuracy: 0.8668\n",
            "Epoch 126/150\n",
            "8/8 [==============================] - 6s 754ms/step - loss: 0.4436 - accuracy: 0.8607\n",
            "Epoch 127/150\n",
            "8/8 [==============================] - 6s 748ms/step - loss: 0.4195 - accuracy: 0.8648\n",
            "Epoch 128/150\n",
            "8/8 [==============================] - 6s 813ms/step - loss: 0.3683 - accuracy: 0.8852\n",
            "Epoch 129/150\n",
            "8/8 [==============================] - 6s 750ms/step - loss: 0.4247 - accuracy: 0.8668\n",
            "Epoch 130/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.3654 - accuracy: 0.8832\n",
            "Epoch 131/150\n",
            "8/8 [==============================] - 6s 762ms/step - loss: 0.3056 - accuracy: 0.9119\n",
            "Epoch 132/150\n",
            "8/8 [==============================] - 6s 758ms/step - loss: 0.4467 - accuracy: 0.8545\n",
            "Epoch 133/150\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 0.3134 - accuracy: 0.8873\n",
            "Epoch 134/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.4071 - accuracy: 0.8914\n",
            "Epoch 135/150\n",
            "8/8 [==============================] - 7s 812ms/step - loss: 0.3493 - accuracy: 0.8832\n",
            "Epoch 136/150\n",
            "8/8 [==============================] - 6s 765ms/step - loss: 0.3397 - accuracy: 0.8770\n",
            "Epoch 137/150\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 0.3240 - accuracy: 0.9078\n",
            "Epoch 138/150\n",
            "8/8 [==============================] - 7s 773ms/step - loss: 0.3212 - accuracy: 0.8811\n",
            "Epoch 139/150\n",
            "8/8 [==============================] - 6s 773ms/step - loss: 0.4151 - accuracy: 0.8627\n",
            "Epoch 140/150\n",
            "8/8 [==============================] - 7s 790ms/step - loss: 0.3511 - accuracy: 0.8852\n",
            "Epoch 141/150\n",
            "8/8 [==============================] - 7s 768ms/step - loss: 0.3506 - accuracy: 0.8873\n",
            "Epoch 142/150\n",
            "8/8 [==============================] - 6s 767ms/step - loss: 0.3216 - accuracy: 0.9016\n",
            "Epoch 143/150\n",
            "8/8 [==============================] - 6s 769ms/step - loss: 0.3037 - accuracy: 0.9098\n",
            "Epoch 144/150\n",
            "8/8 [==============================] - 6s 765ms/step - loss: 0.3267 - accuracy: 0.9078\n",
            "Epoch 145/150\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.3727 - accuracy: 0.8852\n",
            "Epoch 146/150\n",
            "8/8 [==============================] - 6s 764ms/step - loss: 0.3506 - accuracy: 0.8975\n",
            "Epoch 147/150\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 0.2997 - accuracy: 0.8934\n",
            "Epoch 148/150\n",
            "8/8 [==============================] - 6s 755ms/step - loss: 0.3930 - accuracy: 0.9016\n",
            "Epoch 149/150\n",
            "8/8 [==============================] - 6s 766ms/step - loss: 0.3466 - accuracy: 0.8852\n",
            "Epoch 150/150\n",
            "8/8 [==============================] - 6s 764ms/step - loss: 0.2588 - accuracy: 0.9303\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15f4382fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터 측정\n",
        "\n",
        "model7.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_0PEZEKoIr3",
        "outputId": "6f99fbae-e0bc-4c42-bb56-edb06b2e92ab"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 104ms/step - loss: 3.1128 - accuracy: 0.6992\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.1128273010253906, 0.6991869807243347]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 이미지를 넣기 위해 기존 폴더로 이동\n",
        "\n",
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "ENeNz08bDhBc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 이미지 넣기\n",
        "\n",
        "test1 = Image.open('/content/테스트3.jpg').convert('RGB')\n",
        "test1 = test1.resize((224, 224))\n",
        "test1 = np.array(test1)\n",
        "test1 = test1.reshape(1,224,224,3)\n",
        "\n",
        "\n",
        "print(test1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaU7clcgDxnQ",
        "outputId": "0dfdecae-ca9e-4ea0-cd86-a2cf8f4f9f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 돌리기\n",
        "\n",
        "ans1 = model.predict(test1)\n",
        "print(folder_name)\n",
        "\n",
        "print(model.predict(test1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgKrESZpDL4v",
        "outputId": "ee2652f3-9a8d-45b3-a03c-3905824cb766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['르라보_상탈', '마르지엘라_커피브레이크', '샤넬_블루드', '이솝_테싯', '조말론_네롤리', '아쿠아디파르마_미르토', '딥디크_도손', '톰포드_화이트 스웨이드', '존바바토스_아티산', '크리드_로얄워터', '바이레도_집시워터', '에르메스_운 자르뎅 수르닐']\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#정확도 상위 3개와 정확도 출력\n",
        "\n",
        "ans1 = model.predict(test1)\n",
        "print(folder_name)\n",
        "\n",
        "print(model.predict(test1))\n",
        "preds = model.predict(test1)\n",
        "p = np.argsort(preds, axis=1)\n",
        "print(np.flip(p))\n",
        "p1 = np.flip((p))\n",
        "print(type(p))\n",
        "\n",
        "for i in range(3):\n",
        "  print(folder_name[int(p1[0][i])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyNVLGP7Rijp",
        "outputId": "7a08a25c-24f2-4640-8d7d-e0c14533c6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['르라보_상탈', '마르지엘라_커피브레이크', '샤넬_블루드', '이솝_테싯', '조말론_네롤리', '아쿠아디파르마_미르토', '딥디크_도손', '톰포드_화이트 스웨이드', '존바바토스_아티산', '크리드_로얄워터', '바이레도_집시워터', '에르메스_운 자르뎅 수르닐']\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "[[11 10  9  8  7  6  5  4  3  2  1  0]]\n",
            "<class 'numpy.ndarray'>\n",
            "에르메스_운 자르뎅 수르닐\n",
            "바이레도_집시워터\n",
            "크리드_로얄워터\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save(\"test_model\")"
      ],
      "metadata": {
        "id": "ZvKO85xvYoka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4276e713-1378-4426-dce4-884228f31a2e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: test_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mq8HjFZHNDP3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cnn_test(1차테스트_종합).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}